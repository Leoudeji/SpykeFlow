{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* This notebook has implementations of `Backprop` with binary spike feature vectors obtained from the `SPIKEFLOW`. Here, we will use tf.data API. Here the shape of the inputs is `[None, n_input]` instead of `[n_input, None]`. We had to do this because nested elements in `from_tensor_slices` must have the same dimension in 0th rank [see](https://stackoverflow.com/questions/49579684/what-is-the-difference-between-dataset-from-tensors-and-dataset-from-tensor-slic). Everytime an iterator `iter = dataset.make_initializable_iterator()` gets initialized, the dataset is randomly shuffled so we need not shuffle again, [see](https://stackoverflow.com/questions/49579684/what-is-the-difference-between-dataset-from-tensors-and-dataset-from-tensor-slic). We also use `z_3 = tf.floor(z_3)`. Surrogate gradients with one step is used. (one sided)\n",
    "\n",
    "* Here, error in the hidden layer, $\\delta^{2}$ is implemented as:\n",
    "$ \\delta^{2} = W^{3T}\\delta^{(3)}\\odot\\sigma^{'}(z^{(2)}) \\tag{1}$\n",
    "* $\\sigma^{'}(z^{(2)})$ is approximated with a surrogate. (See section 6)\n",
    "* It also takes care of catastrophic forgetting by using synaptic intelligence.\n",
    "* He initialization without AR1 gives lesser final accuracy than Trunc init because in He the gradients don't suffer from dimnishing.\n",
    "## References\n",
    "* [Neural Nets](http://neuralnetworksanddeeplearning.com/chap3.html)\n",
    "* [Randombackprop](https://github.com/xuexue/randombp/blob/master/randombp.py)\n",
    "* [Randombackprop](https://github.com/sangyi92/feedback_alignment/blob/master/RFA.ipynb)\n",
    "* [Backprop](http://blog.aloni.org/posts/backprop-with-tensorflow/)\n",
    "* [Initializers](https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404)\n",
    "* [Dropout](https://github.com/pinae/TensorFlow-MNIST-example/blob/master/fully-connected.py)\n",
    "* [Softmax](https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits)\n",
    "* [SoftmaxLogits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* [TF memory leaks when  assigning in loop](https://github.com/tensorflow/tensorflow/issues/4151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, HTML\n",
    "tf.compat.v2.random.set_seed(0)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.client import timeline\n",
    "import h5py, pickle\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import DATA_Loader\n",
    "import seaborn as sb\n",
    "import theano, random, sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow' from '/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/__init__.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print(tf.__version__)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hide code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "if (code_show){\n",
       "$('div.input').hide();\n",
       "} else {\n",
       "$('div.input').show();\n",
       "}\n",
       "code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpl.rcParams['figure.figsize'] = 15,10\n",
    "mpl.rcParams['axes.titlesize'] = 24\n",
    "mpl.rcParams['axes.labelsize'] = 25\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['lines.markersize'] = 10\n",
    "mpl.rcParams['xtick.labelsize'] = 30\n",
    "mpl.rcParams['ytick.labelsize'] = 22\n",
    "mpl.rcParams['legend.fontsize'] = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = 3.75,3\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['lines.markersize'] = 10\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train features:60000\n",
      "Total test features:10000\n"
     ]
    }
   ],
   "source": [
    "filename = '../../spiking_networks/train_pool1_spike_features_inh_False_conv1maps_30.h5'\n",
    "with h5py.File(filename, 'r') as hf:\n",
    "    emnist_train_images = hf['pool1_spike_features'][:].astype(np.int8)\n",
    "emnist_train_images[np.where(emnist_train_images>=1)] = 1\n",
    "\n",
    "filehandle = open('../../spiking_networks/train_y.pkl','rb')\n",
    "emnist_train_labels = pickle.load(filehandle).astype(np.int).tolist()\n",
    "filehandle.close()\n",
    "emnist_train_labels = np.array(emnist_train_labels)\n",
    "print('Total train features:{}'.format(emnist_train_images.shape[0]))\n",
    "\n",
    "#### LOAD TEST IMAGES AND LABELS\n",
    "filename = '../../spiking_networks/test_pool1_spike_features_inh_False_conv1maps_30.h5'\n",
    "with h5py.File(filename, 'r') as hf:\n",
    "    emnist_test_images = hf['pool1_spike_features'][:].astype(np.int8)\n",
    "emnist_test_images[np.where(emnist_test_images>=1)] = 1\n",
    "print('Total test features:{}'.format(emnist_test_images.shape[0]))\n",
    "\n",
    "filehandle = open('../../spiking_networks/test_y.pkl','rb')\n",
    "emnist_test_labels = pickle.load(filehandle).astype(np.int)\n",
    "filehandle.close()\n",
    "emnist_test_labels = np.array(emnist_test_labels)\n",
    "\n",
    "#### LOAD TRAIN AND VALIDATION DATA AND LABELS\n",
    "train_images = emnist_train_images\n",
    "train_labels = emnist_train_labels\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = emnist_test_images\n",
    "test_labels = emnist_test_labels\n",
    "num_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0:'0',1:'1',2:'2',3:'3',4:'4',5:'5',6:'6',7:'7',8:'8',9:'9',10:'A',11:'B',12:'C',13:'D',14:'E',15:'F',\n",
    "              16:'G',17:'H',18:'I',19:'J',20:'K',21:'L',22:'M',23:'N',24:'O',25:'P',26:'Q',27:'R',28:'S',29:'T',30:'U',\n",
    "             31:'V',32:'W',33:'X',34:'Y',35:'Z',36:'a',37:'b',38:'d',39:'e',40:'f',41:'g',42:'h',43:'n',44:'q',45:'r',\n",
    "             46:'t'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "#### EXTRACT REQUIRED LOCATIONS OF 0 TO 5 FOR TRAIN DATA\n",
    "def extract_class_data(start=0, stop=1):\n",
    "    set1_locs = np.where((train_labels>=start) & (train_labels<=stop))[0]\n",
    "    train_labels_set1 = to_categorical(train_labels[set1_locs], num_classes=num_classes)\n",
    "    train_images_set1 = train_images[set1_locs,:]\n",
    "    n_images = len(train_images_set1)\n",
    "\n",
    "    #### EXTRACT REQUIRED LOCATIONS OF 0 TO 5 FOR TEST DATA\n",
    "    set1_locs = np.where((test_labels>=start) & (test_labels<=stop))[0]\n",
    "    test_labels_set1 = to_categorical(test_labels[set1_locs], num_classes=num_classes)\n",
    "    test_images_set1 = test_images[set1_locs,:]\n",
    "    print('Test features:{}'.format(test_images_set1.shape))\n",
    "    print('Length of test labels:{}'.format(test_labels_set1.shape[0]))\n",
    "    test_data_set1 = (test_images_set1, test_labels_set1)\n",
    "    \n",
    "\n",
    "\n",
    "    train_images_set1 = train_images_set1[int(0.09*n_images):]\n",
    "    train_labels_set1 = train_labels_set1[int(0.09*n_images):]\n",
    "    print('Train features:{}'.format(train_images_set1.shape))\n",
    "    print('Length of train labels:{}'.format(train_labels_set1.shape[0]))\n",
    "    train_data_set1 = (train_images_set1, train_labels_set1)\n",
    "\n",
    "    valid_labels_set1 = train_labels_set1[0:int(0.09*n_images)]\n",
    "    valid_images_set1 = train_images_set1[0:int(0.09*n_images)]\n",
    "    print('Valid features:{}'.format(valid_images_set1.shape))\n",
    "    print('Length of valid labels:{}'.format(valid_labels_set1.shape[0]))\n",
    "    valid_data_set1 = (valid_images_set1, valid_labels_set1)\n",
    "    \n",
    "    n_train_set1 = train_labels_set1.shape[0]\n",
    "    n_test_set1 = test_labels_set1.shape[0]\n",
    "    n_valid_set1 = valid_labels_set1.shape[0]\n",
    "\n",
    "    return train_data_set1, valid_data_set1, test_data_set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess= tf.InteractiveSession(config=config)\n",
    "run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "run_metadata = tf.RunMetadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tanh(x) = \\frac{(e^{x} – e^{-x})}{(e^{x} + e^{-x})}$\n",
    "\n",
    "$d\\frac{tanh(x)}{dx} = 1 – (tanh(x))^{2}$\n",
    "\n",
    "$\\sigma(x) = \\frac{1.0}{1 + e^{-x}}$\n",
    "\n",
    "$d\\frac{\\sigma(x)}{dx} = \\sigma(x)*(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 3630\n",
    "n_middle = 1024\n",
    "n_out = 10\n",
    "batch_size = tf.placeholder(tf.int64, name='batch_size') \n",
    "a_1 = tf.placeholder(tf.float32, [None, n_input], name = 'Input_batch')\n",
    "y = tf.placeholder(tf.float32, [None, n_out], name = 'output_batch')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((a_1, y))\n",
    "#dataset = dataset.shuffle(buffer_size=len(all_train_labels), reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "drop_out = tf.placeholder(tf.float32)\n",
    "tau = tf.placeholder(tf.float32)\n",
    "set1_mask = tf.placeholder(tf.float32, [10], name='mask')\n",
    "eta = tf.placeholder(tf.float32)\n",
    "n_tot = tf.placeholder(tf.float32)\n",
    "lmbda = tf.placeholder(tf.float32, name='lambda')\n",
    "with tf.name_scope('hid_lyr_w_b'):  ###havier or glorot initialization\n",
    "    low = -4*tf.math.sqrt(6.0/(n_input + n_middle)) # use 4 for sigmoid, 1 for tanh activation \n",
    "    high = 4*tf.math.sqrt(6.0/(n_input + n_middle))\n",
    "    \n",
    "    low = -tf.math.sqrt(2.0/(n_input)) # use 4 for sigmoid, 1 for tanh activation \n",
    "    high = tf.math.sqrt(2.0/(n_input))\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_uniform(shape=[n_input,n_middle],minval=low,maxval=high), name = 'W_2')\n",
    "    #w_2 = tf.Variable(tf.truncated_normal(shape=[n_input,n_middle], stddev=0.01),name = 'W_2')\n",
    "    w_2_update_placeholder = tf.placeholder(w_2.dtype, shape=w_2.get_shape(),name='update_w2')\n",
    "    w_2_update_op = w_2.assign(w_2_update_placeholder)\n",
    "    tf.summary.histogram('w_2', w_2)\n",
    "    \n",
    "    b_2 = tf.Variable(tf.zeros([1,n_middle]), name = 'b_2')\n",
    "    b_2_update_placeholder = tf.placeholder(b_2.dtype, shape=b_2.get_shape(),name='update_b2')\n",
    "    b_2_update_op = b_2.assign(b_2_update_placeholder)\n",
    "    tf.summary.histogram('b_2', b_2)\n",
    "    \n",
    "    \n",
    "    w2_grad_accum = tf.Variable(np.zeros(shape=[n_input,n_middle], dtype=np.float32),name='w2_grad_accum')\n",
    "    w2_grad_accum_update_placeholder=tf.placeholder(w2_grad_accum.dtype, shape=w2_grad_accum.get_shape(),\n",
    "                                            name='update_w2_grad_accum')\n",
    "    w2_grad_accum_update_op = w2_grad_accum.assign(w2_grad_accum_update_placeholder)\n",
    "    \n",
    "    \n",
    "    b2_grad_accum = tf.Variable(np.zeros(shape=[1,n_middle], dtype=np.float32), name='b2_grad_accum')\n",
    "    b2_grad_accum_update_placeholder=tf.placeholder(b2_grad_accum.dtype,shape=b2_grad_accum.get_shape(),\n",
    "                                            name='update_b2_grad_accum')\n",
    "    b2_grad_accum_update_op = b2_grad_accum.assign(b2_grad_accum_update_placeholder)\n",
    "    \n",
    "    \n",
    "    big_omeg_w2 = tf.Variable(np.zeros(shape=[n_input,n_middle], dtype=np.float32), name='omeg_w2')\n",
    "    tf.summary.histogram('big_omeg_w2', big_omeg_w2)\n",
    "    big_omeg_w2_update_placeholder = tf.placeholder(big_omeg_w2.dtype, shape=big_omeg_w2.get_shape(),\n",
    "                                                    name='update_big_omeg_w2')\n",
    "    big_omeg_w2_update_op = big_omeg_w2.assign(big_omeg_w2_update_placeholder)\n",
    "    \n",
    "    \n",
    "    big_omeg_b2 = tf.Variable(np.zeros(shape=[1,n_middle], dtype=np.float32), name='omeg_b2')\n",
    "    tf.summary.histogram('big_omeg_b2', big_omeg_b2)\n",
    "    big_omeg_b2_update_placeholder = tf.placeholder(big_omeg_b2.dtype, shape=big_omeg_b2.get_shape(),\n",
    "                                                    name='update_big_omeg_b2')\n",
    "    big_omeg_b2_update_op = big_omeg_b2.assign(big_omeg_b2_update_placeholder)\n",
    "    \n",
    "    \n",
    "    star_w2 = tf.Variable(np.zeros(shape=[n_input,n_middle], dtype=np.float32), name='star_w2')\n",
    "    star_w2_update_placeholder = tf.placeholder(star_w2.dtype, shape=star_w2.get_shape(),\n",
    "                                                name='update_star_w2')\n",
    "    star_w2_update_op = star_w2.assign(star_w2_update_placeholder)\n",
    "    \n",
    "    \n",
    "    star_b2 = tf.Variable(np.zeros(shape=[1,n_middle], dtype=np.float32), name='star_b2')\n",
    "    star_b2_update_placeholder = tf.placeholder(star_b2.dtype, shape=star_b2.get_shape(),\n",
    "                                                name='update_star_b2')\n",
    "    star_b2_update_op = star_b2.assign(star_b2_update_placeholder)\n",
    "with tf.name_scope('op_lyr_w_b'):\n",
    "    \n",
    "    low = -tf.math.sqrt(2.0/(n_middle))\n",
    "    high = tf.math.sqrt(2.0/(n_middle))\n",
    "    #w_3 = tf.Variable(tf.random_uniform(shape=[n_middle,10],minval=low,maxval=high), name = 'W_3')\n",
    "    #w_3 = tf.Variable(tf.truncated_normal(shape=[n_middle,n_out], stddev=0.01),name = 'W_3')\n",
    "    w_3 = tf.Variable(np.zeros(shape=[n_middle,n_out], dtype=np.float32),name = 'W_3')\n",
    "    tf.summary.histogram('w_3', w_3)\n",
    "    w_3_update_placeholder = tf.placeholder(w_3.dtype, shape=w_3.get_shape(),name='update_w3')\n",
    "    w_3_update_op = w_3.assign(w_3_update_placeholder)\n",
    "    \n",
    "    b_3 = tf.Variable(tf.zeros([1,n_out]), name = 'b_3')\n",
    "    \n",
    "    b_3_update_placeholder = tf.placeholder(b_3.dtype, shape=b_3.get_shape(),name='update_b3')\n",
    "    b_3_update_op = b_3.assign(b_3_update_placeholder)\n",
    "    \n",
    "    w3_grad_accum = tf.Variable(np.zeros(shape=[n_middle,n_out], dtype=np.float32), name='w3_grad_accum')\n",
    "    b3_grad_accum = tf.Variable(np.zeros(shape=[1,n_out], dtype=np.float32), name='b3_grad_accum')\n",
    "    \n",
    "    big_omeg_w3 = tf.Variable(np.zeros(shape=[n_middle,n_out], dtype=np.float32), name='omeg_w3')\n",
    "    tf.summary.histogram('big_omeg_w3', big_omeg_w3)\n",
    "    big_omeg_b3 = tf.Variable(np.zeros(shape=[1,n_out], dtype=np.float32), name='omeg_b3')\n",
    "    tf.summary.histogram('big_omeg_b3', big_omeg_b3)\n",
    "    \n",
    "    star_w3 = tf.Variable(np.zeros(shape=[n_middle,n_out], dtype=np.float32), name='star_w3')\n",
    "    star_b3 = tf.Variable(np.zeros(shape=[1,n_out], dtype=np.float32), name='star_b3')\n",
    "\n",
    "def sigma(x):\n",
    "    return tf.math.divide(tf.constant(1.0),\n",
    "                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "def tanh(x):\n",
    "    return tf.math.divide(tf.subtract(tf.exp(x), tf.exp(tf.negative(x))), \n",
    "                          tf.add(tf.exp(x), tf.exp(tf.negative(x))) )\n",
    "\n",
    "def sigmaprime(x):\n",
    "    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))\n",
    "\n",
    "def tanhprime(x):\n",
    "    return tf.subtract(tf.constant(1.0),tf.square(tanh(x)))\n",
    "\n",
    "def spkNeuron(x):\n",
    "    return tf.where(tf.greater_equal(x,0.0), tf.ones_like(x), \n",
    "                        tf.zeros_like(x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return tf.maximum(0.0, x)\n",
    "\n",
    "def ReLUprime(x):\n",
    "    return tf.where(tf.greater_equal(x,0.0), tf.ones_like(x), \n",
    "                        tf.zeros_like(x))\n",
    "\n",
    "def spkPrime1(x):\n",
    "    l1_bound_higher = tf.greater_equal(x,-0/4)\n",
    "    r1_bound_lesser = tf.less_equal(x,tau/4) \n",
    "    grad_one = tf.where(tf.logical_and(l1_bound_higher,r1_bound_lesser), tf.ones_like(x), tf.zeros_like(x))\n",
    "    return grad_one\n",
    "\n",
    "def firstLyrSpks(x):\n",
    "    return tf.where(tf.greater_equal(x,1.0), tf.ones_like(x), \n",
    "                        tf.zeros_like(x))\n",
    "\n",
    "    \n",
    "with tf.name_scope('hid_lyr_acti'):\n",
    "    z_2 = tf.add(tf.matmul(features,w_2,name = 'w_2xa_1'), b_2, name = 'z_2')\n",
    "    locs_to_drop = tf.random.categorical(tf.math.log([[1.0-drop_out, drop_out]]), tf.size(z_2))\n",
    "    locs_to_drop = tf.reshape(locs_to_drop, tf.shape(z_2))\n",
    "    z_2 = tf.where(locs_to_drop>0,-tf.ones_like(z_2),z_2, 'drop_out_app')\n",
    "    tf.summary.histogram('z_2', z_2)\n",
    "    #@a_2 = ReLU(z_2)\n",
    "    a_2 = spkNeuron(z_2)\n",
    "    tf.summary.histogram('a_2', a_2)\n",
    "with tf.name_scope('op_lyr_acti'):\n",
    "    z_3 = tf.add(tf.matmul(a_2,w_3, name = 'w_3xa_2'),b_3, name = 'z_3')\n",
    "    z_3 = tf.floor(z_3)\n",
    "    #z_3 = tf.subtract(tf.reduce_max(z_3),z_3, name = 'inhibition')\n",
    "    tf.summary.histogram('z_3', z_3)\n",
    "    #@a_3  = sigma(z_3) ##UNCOMMENT THIS LINE AND COMMENT ABOVE LINE IF YOU WANT spike SQUISHING\n",
    "    a_3 = tf.cast(tf.nn.softmax(z_3,axis=1), tf.float32)\n",
    "    a_3 = tf.multiply(a_3, set1_mask, name='masking')\n",
    "    tf.summary.histogram('a_3', a_3)\n",
    "    ##COMMENT THE ABOVE LINE AND UNCOMMENT BELOW LINE IF YOU WANT SOFTMAX\n",
    "    #a_3 = tf.nn.softmax(z_3,axis=1) ##AXIS IS VERY IMPORTANT!!! axis=1 INDICATES THE CLASSES AS y IS [None,10]\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum((y*tf.log(a_3) +tf.log(1-a_3)*(1-y)) ,axis=0), name = 'cost_calc') WORKS, USE BELOW\n",
    "with tf.name_scope('cost_calc'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,logits=z_3,axis=1),\n",
    "                          name = 'cost_calc')#WORKS\n",
    "        ##COMMENT BELOW LINES IF YOU WANT quadratic\n",
    "    #@dc_da = tf.multiply(-tf.subtract(labels,a_3, name = 'y_minus_a_3'), mask)\n",
    "    #@cost = tf.reduce_mean(tf.reduce_sum((1/2.0)*tf.square(dc_da),axis=1), name = 'cost_calc')\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "with tf.name_scope('op_lyr_grad'):\n",
    "    #@d_z_3 = tf.multiply(-tf.subtract(labels,a_3, name = 'delta3'), mask, name='masking')\n",
    "    d_z_3 = -tf.subtract(labels,a_3, name = 'delta3')\n",
    "    #d_z_3 = tf.multiply(dc_da,a_3, name = 'delta3')\n",
    "    d_b_3 = tf.expand_dims(tf.reduce_mean(d_z_3, axis=[0]), axis=0)\n",
    "    tf.summary.histogram('d_b_3', d_b_3)\n",
    "    d_w_3 = tf.multiply(1/tf.cast(batch_size, tf.float32),\n",
    "                        tf.matmul(tf.transpose(a_2),d_z_3), \n",
    "                        name='delta_w3')\n",
    "    tf.summary.histogram('d_w_3', d_w_3)\n",
    "    \n",
    "with tf.name_scope('hid_lyr_grad'):\n",
    "    #@d_z_2 = tf.multiply(tf.matmul(d_z_3,tf.transpose(w_3), name = 'w_3Txdelta3'), ReLUprime(z_2),\n",
    "    #@                    name = 'delta2')\n",
    "    d_z_2 = tf.multiply(tf.matmul(d_z_3,tf.transpose(w_3), name = 'w_3Txdelta3'), spkPrime1(z_2),\n",
    "                        name = 'delta2')\n",
    "    #d_z_2 = tf.matmul(d_z_3,tf.transpose(w_3), name = 'delta2')\n",
    "    d_b_2 = tf.expand_dims(tf.reduce_mean(d_z_2, axis=[0]), axis=0)\n",
    "    tf.summary.histogram('d_b_2', d_b_2)\n",
    "    d_w_2 = tf.multiply(1/tf.cast(batch_size, tf.float32),\n",
    "                        tf.matmul(tf.transpose(features),d_z_2), \n",
    "                        name='delta_w2')\n",
    "    tf.summary.histogram('d_w_2', d_w_2)\n",
    "    \n",
    "omega_step=[tf.assign(w2_grad_accum,\n",
    "                      tf.add(w2_grad_accum,tf.multiply(eta*eta*lmbda/n_tot, tf.square(d_w_2))),\n",
    "                     name='update_omeg_w2'),\n",
    "            tf.assign(b2_grad_accum,\n",
    "                      tf.add(b2_grad_accum,tf.multiply(eta*eta*lmbda/n_tot,tf.square(d_b_2))),\n",
    "                     name='update_omeg_b2')\n",
    "]\n",
    "\n",
    "step = [tf.assign(w_2,\n",
    "                  tf.subtract(w_2, (eta*d_w_2+big_omeg_w2*(w_2-star_w2))),name='update_w_2'),\n",
    "        tf.assign(b_2,\n",
    "                  tf.subtract(b_2,(eta*d_b_2+big_omeg_b2*(b_2-star_b2))),name='update_b_2'),\n",
    "        \n",
    "        tf.assign(w_3,\n",
    "                  tf.subtract(w_3, (eta*d_w_3+big_omeg_w3*(w_3-star_w3))),name='update_w_3'),\n",
    "        tf.assign(b_3,\n",
    "                  tf.subtract(b_3,(eta*d_b_3+big_omeg_b3*(b_3-star_b3))),name='update_b_3')    \n",
    "]\n",
    "with tf.name_scope('acc_calc'):\n",
    "    predictions = tf.argmax(a_3, 1)\n",
    "    acct_mat = tf.equal(tf.argmax(a_3, 1), tf.argmax(labels, 1))\n",
    "    acct_res = tf.reduce_mean(tf.cast(acct_mat, tf.float32))\n",
    "    tf.summary.scalar('accuracy', acct_res)\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the writer with $\\lambda$s (0, 6.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ruthvik/Desktop/Summer 2017/tf_graph_outputs/mnist/continual_learning/original_mnist_5sets'\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(path + '/spike_sh_16lmbdas_ar1_3lyrs_he1', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate $\\lambda$s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.66798336 1.87899335 2.86549374 5.70223889 0.03185228 0.82059157\n",
      " 4.52755632 5.5745061  0.92276948 3.88187997 6.01642319 1.41211432\n",
      " 1.25096548 0.73154401 1.48295808]\n",
      "()\n",
      "[  318.52279289  7315.44010634  8205.91565286  9227.69480373\n",
      " 12509.65481963 14121.14324291 14829.58075219 18789.93349383\n",
      " 28654.93737557 36679.83357089 38818.79973634 45275.56321906\n",
      " 55745.06096959 57022.38893159 60164.23191608]\n",
      "()\n",
      "([0, 318.52279289064813, 7315.4401063372225, 8205.915652860209, 9227.694803734325, 12509.654819630066, 14121.143242910297, 14829.58075218697, 18789.933493831242, 28654.937375566486, 36679.833570890165, 38818.79973633937, 45275.563219057556, 55745.06096959071, 57022.388931593494, 60164.23191607782, 0, 318.52279289064813, 7315.4401063372225, 8205.915652860209, 9227.694803734325, 12509.654819630066, 14121.143242910297, 14829.58075218697, 18789.933493831242, 28654.937375566486, 36679.833570890165, 38818.79973633937, 45275.563219057556, 55745.06096959071, 57022.388931593494, 60164.23191607782, 0, 318.52279289064813, 7315.4401063372225, 8205.915652860209, 9227.694803734325, 12509.654819630066, 14121.143242910297, 14829.58075218697, 18789.933493831242, 28654.937375566486, 36679.833570890165, 38818.79973633937, 45275.563219057556, 55745.06096959071, 57022.388931593494, 60164.23191607782, 0, 318.52279289064813, 7315.4401063372225, 8205.915652860209, 9227.694803734325, 12509.654819630066, 14121.143242910297, 14829.58075218697, 18789.933493831242, 28654.937375566486, 36679.833570890165, 38818.79973633937, 45275.563219057556, 55745.06096959071, 57022.388931593494, 60164.23191607782, 0, 318.52279289064813, 7315.4401063372225, 8205.915652860209, 9227.694803734325, 12509.654819630066, 14121.143242910297, 14829.58075218697, 18789.933493831242, 28654.937375566486, 36679.833570890165, 38818.79973633937, 45275.563219057556, 55745.06096959071, 57022.388931593494, 60164.23191607782], 80)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "n_lmbdas = 15\n",
    "a = np.random.uniform(low=0.0, high=6.75, size=(n_lmbdas,))\n",
    "print(a)\n",
    "print()\n",
    "a = a*1.0e4\n",
    "a = a[np.argsort(a)]\n",
    "print(a)\n",
    "print()\n",
    "n_reps = 5\n",
    "a = ([0]+a.tolist())*n_reps\n",
    "n_lmbdas+=1\n",
    "print(a, len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80)\n"
     ]
    }
   ],
   "source": [
    "lmbdas = a\n",
    "np.random.seed(0)\n",
    "np_weights = []\n",
    "low = -np.sqrt(2.0/(n_input)) # use 4 for sigmoid, 1 for tanh activation \n",
    "high = np.sqrt(2.0/(n_input))\n",
    "for i in range(n_reps):\n",
    "    np_weights.append(np.random.uniform(low=low,high=high,size=(n_input,n_middle)))\n",
    "\n",
    "np_weights=[item for item in np_weights for i in range(n_lmbdas)]\n",
    "print(len(np_weights), len(lmbdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np_weights[0], np_weights[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np_weights[0], np_weights[16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commence training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lmbda:0, 0\n",
      "Current mask:[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Test features:(2115, 3630)\n",
      "Length of test labels:2115\n",
      "Train features:(11526, 3630)\n",
      "Length of train labels:11526\n",
      "Valid features:(1139, 3630)\n",
      "Length of valid labels:1139\n",
      "Repeat:0\n",
      "training cost:2.30258536339 and training accuracy:0.467811912298\n",
      "validation cost:2.30258536339 and validation accuracy:0.473222136497\n",
      "Training on :(0, 1)\n",
      "training cost:0.0424780659378 and training accuracy:0.99514144659\n",
      "validation cost:0.0425861291587 and validation accuracy:0.996488153934\n",
      "Training on :(0, 1)\n",
      "training cost:0.0238319151103 and training accuracy:0.995575249195\n",
      "validation cost:0.0244052931666 and validation accuracy:0.997366130352\n",
      "Training on :(0, 1)\n",
      "training cost:0.0170864723623 and training accuracy:0.996703088284\n",
      "validation cost:0.0173231437802 and validation accuracy:0.996488153934\n",
      "Training on :(0, 1)\n",
      "training cost:0.0137033909559 and training accuracy:0.997136890888\n",
      "validation cost:0.0140941208228 and validation accuracy:0.996488153934\n",
      "Training on :(0, 1)\n",
      "training cost:0.0115019194782 and training accuracy:0.997657477856\n",
      "validation cost:0.0115101812407 and validation accuracy:0.997366130352\n",
      "Training on :(0, 1)\n",
      "training cost:0.0099402749911 and training accuracy:0.997744202614\n",
      "validation cost:0.00971961114556 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "training cost:0.00863864738494 and training accuracy:0.998004496098\n",
      "validation cost:0.0086479568854 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "training cost:0.00776934204623 and training accuracy:0.99809128046\n",
      "validation cost:0.0084388460964 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "training cost:0.00688597885892 and training accuracy:0.998178005219\n",
      "validation cost:0.00751441437751 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "Time taken:130.874908924\n",
      "Current mask:[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Test features:(2042, 3630)\n",
      "Length of test labels:2042\n",
      "Train features:(11001, 3630)\n",
      "Length of train labels:11001\n",
      "Valid features:(1088, 3630)\n",
      "Length of valid labels:1088\n",
      "Repeat:0\n",
      "training cost:2.30258536339 and training accuracy:0.0\n",
      "validation cost:2.30258536339 and validation accuracy:0.0\n",
      "Training on :(2, 3)\n",
      "training cost:0.168302044272 and training accuracy:0.950004518032\n",
      "validation cost:0.169079318643 and validation accuracy:0.943014681339\n",
      "Training on :(2, 3)\n",
      "training cost:0.113371483982 and training accuracy:0.968548297882\n",
      "validation cost:0.12098506093 and validation accuracy:0.962316155434\n",
      "Training on :(2, 3)\n",
      "training cost:0.0885975807905 and training accuracy:0.97309333086\n",
      "validation cost:0.0926486998796 and validation accuracy:0.967830896378\n",
      "Training on :(2, 3)\n",
      "training cost:0.0738820210099 and training accuracy:0.974093258381\n",
      "validation cost:0.0787701904774 and validation accuracy:0.96875\n",
      "Training on :(2, 3)\n",
      "training cost:0.0646886602044 and training accuracy:0.977183878422\n",
      "validation cost:0.0672900676727 and validation accuracy:0.971507370472\n",
      "Training on :(2, 3)\n",
      "training cost:0.0566079691052 and training accuracy:0.9806381464\n",
      "validation cost:0.0583092048764 and validation accuracy:0.976102948189\n",
      "Training on :(2, 3)\n",
      "training cost:0.0513964854181 and training accuracy:0.982819736004\n",
      "validation cost:0.0509350299835 and validation accuracy:0.980698525906\n",
      "Training on :(2, 3)\n",
      "training cost:0.0464820750058 and training accuracy:0.985546767712\n",
      "validation cost:0.0469166785479 and validation accuracy:0.984375\n",
      "Training on :(2, 3)\n",
      "training cost:0.0427470132709 and training accuracy:0.987273871899\n",
      "validation cost:0.0414676591754 and validation accuracy:0.987132370472\n",
      "Training on :(2, 3)\n",
      "Time taken:122.635829926\n",
      "Current mask:[1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "Test features:(1874, 3630)\n",
      "Length of test labels:1874\n",
      "Train features:(10250, 3630)\n",
      "Length of train labels:10250\n",
      "Valid features:(1013, 3630)\n",
      "Length of valid labels:1013\n",
      "Repeat:0\n",
      "training cost:2.30258536339 and training accuracy:0.0\n",
      "validation cost:2.30258536339 and validation accuracy:0.0\n",
      "Training on :(4, 5)\n",
      "training cost:0.0990518555045 and training accuracy:0.991609752178\n",
      "validation cost:0.0925189256668 and validation accuracy:0.99703848362\n",
      "Training on :(4, 5)\n",
      "training cost:0.0539999529719 and training accuracy:0.99287801981\n",
      "validation cost:0.0477138087153 and validation accuracy:0.99703848362\n",
      "Training on :(4, 5)\n",
      "training cost:0.0376890674233 and training accuracy:0.994829297066\n",
      "validation cost:0.0302194636315 and validation accuracy:0.999012827873\n",
      "Training on :(4, 5)\n",
      "training cost:0.0290481522679 and training accuracy:0.99658536911\n",
      "validation cost:0.0220831781626 and validation accuracy:1.0\n",
      "Training on :(4, 5)\n",
      "training cost:0.023657489568 and training accuracy:0.997756123543\n",
      "validation cost:0.0176057368517 and validation accuracy:1.0\n",
      "Training on :(4, 5)\n",
      "training cost:0.0198712404817 and training accuracy:0.998341441154\n",
      "validation cost:0.0145276719704 and validation accuracy:1.0\n",
      "Training on :(4, 5)\n",
      "training cost:0.0169922467321 and training accuracy:0.998341441154\n",
      "validation cost:0.0122214574367 and validation accuracy:1.0\n",
      "Training on :(4, 5)\n",
      "training cost:0.0153050273657 and training accuracy:0.998146355152\n",
      "validation cost:0.011213125661 and validation accuracy:1.0\n",
      "Training on :(4, 5)\n",
      "training cost:0.0139560960233 and training accuracy:0.998243927956\n",
      "validation cost:0.0100094126537 and validation accuracy:1.0\n",
      "Training on :(4, 5)\n",
      "Time taken:130.478960991\n",
      "Current mask:[1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "Test features:(1986, 3630)\n",
      "Length of test labels:1986\n",
      "Train features:(11087, 3630)\n",
      "Length of train labels:11087\n",
      "Valid features:(1096, 3630)\n",
      "Length of valid labels:1096\n",
      "Repeat:0\n",
      "training cost:2.30258536339 and training accuracy:0.0\n",
      "validation cost:2.30258536339 and validation accuracy:0.0\n",
      "Training on :(6, 7)\n",
      "training cost:0.0636208951473 and training accuracy:0.991611778736\n",
      "validation cost:0.0635736957192 and validation accuracy:0.989051103592\n",
      "Training on :(6, 7)\n",
      "training cost:0.0342947021127 and training accuracy:0.996843159199\n",
      "validation cost:0.0337495431304 and validation accuracy:0.995437979698\n",
      "Training on :(6, 7)\n",
      "training cost:0.0241673067212 and training accuracy:0.997745096684\n",
      "validation cost:0.0247216559947 and validation accuracy:0.995437979698\n",
      "Training on :(6, 7)\n",
      "training cost:0.0191191416234 and training accuracy:0.998015701771\n",
      "validation cost:0.01931928657 and validation accuracy:0.996350347996\n",
      "Training on :(6, 7)\n",
      "training cost:0.0155691420659 and training accuracy:0.998286306858\n",
      "validation cost:0.0159959737211 and validation accuracy:0.996350347996\n",
      "Training on :(6, 7)\n",
      "training cost:0.0129639329389 and training accuracy:0.9987372756\n",
      "validation cost:0.0133697763085 and validation accuracy:0.999087572098\n",
      "Training on :(6, 7)\n",
      "training cost:0.011112768203 and training accuracy:0.998827457428\n",
      "validation cost:0.0117992609739 and validation accuracy:0.999087572098\n",
      "Training on :(6, 7)\n",
      "training cost:0.00979762617499 and training accuracy:0.999007821083\n",
      "validation cost:0.0101021369919 and validation accuracy:0.999087572098\n",
      "Training on :(6, 7)\n",
      "training cost:0.00877885892987 and training accuracy:0.999098062515\n",
      "validation cost:0.00852270238101 and validation accuracy:0.999087572098\n",
      "Training on :(6, 7)\n",
      "Time taken:141.758719921\n",
      "Current mask:[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Test features:(1983, 3630)\n",
      "Length of test labels:1983\n",
      "Train features:(10738, 3630)\n",
      "Length of train labels:10738\n",
      "Valid features:(1062, 3630)\n",
      "Length of valid labels:1062\n",
      "Repeat:0\n",
      "training cost:2.30258536339 and training accuracy:0.0\n",
      "validation cost:2.30258536339 and validation accuracy:0.0\n",
      "Training on :(8, 9)\n",
      "training cost:0.151477068663 and training accuracy:0.965170443058\n",
      "validation cost:0.148457437754 and validation accuracy:0.964218437672\n",
      "Training on :(8, 9)\n",
      "training cost:0.103641562164 and training accuracy:0.972527444363\n",
      "validation cost:0.103159971535 and validation accuracy:0.973634660244\n",
      "Training on :(8, 9)\n",
      "training cost:0.082123234868 and training accuracy:0.976066291332\n",
      "validation cost:0.0786714181304 and validation accuracy:0.980225980282\n",
      "Training on :(8, 9)\n",
      "training cost:0.069301366806 and training accuracy:0.979791402817\n",
      "validation cost:0.0627456903458 and validation accuracy:0.98399245739\n",
      "Training on :(8, 9)\n",
      "training cost:0.0595998130739 and training accuracy:0.982957720757\n",
      "validation cost:0.0547399930656 and validation accuracy:0.987758934498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on :(8, 9)\n",
      "training cost:0.0535054244101 and training accuracy:0.985285878181\n",
      "validation cost:0.0488348640501 and validation accuracy:0.990583777428\n",
      "Training on :(8, 9)\n",
      "training cost:0.0479301735759 and training accuracy:0.986775934696\n",
      "validation cost:0.0455383919179 and validation accuracy:0.991525411606\n",
      "Training on :(8, 9)\n",
      "training cost:0.0442771986127 and training accuracy:0.987520933151\n",
      "validation cost:0.0432717837393 and validation accuracy:0.990583777428\n",
      "Training on :(8, 9)\n",
      "training cost:0.0408748500049 and training accuracy:0.988359093666\n",
      "validation cost:0.0389142967761 and validation accuracy:0.993408679962\n",
      "Training on :(8, 9)\n",
      "Time taken:128.926782846\n",
      "Method 3 test accuracy:0.78979998827\n",
      "Training with lmbda:318.522792891, 1\n",
      "Current mask:[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Test features:(2115, 3630)\n",
      "Length of test labels:2115\n",
      "Train features:(11526, 3630)\n",
      "Length of train labels:11526\n",
      "Valid features:(1139, 3630)\n",
      "Length of valid labels:1139\n",
      "Repeat:0\n",
      "training cost:2.30258536339 and training accuracy:0.467811912298\n",
      "validation cost:2.30258536339 and validation accuracy:0.473222136497\n",
      "Training on :(0, 1)\n",
      "training cost:0.0424780659378 and training accuracy:0.99514144659\n",
      "validation cost:0.0425861291587 and validation accuracy:0.996488153934\n",
      "Training on :(0, 1)\n",
      "training cost:0.0238319151103 and training accuracy:0.995575249195\n",
      "validation cost:0.0244052931666 and validation accuracy:0.997366130352\n",
      "Training on :(0, 1)\n",
      "training cost:0.0170864723623 and training accuracy:0.996703088284\n",
      "validation cost:0.0173231437802 and validation accuracy:0.996488153934\n",
      "Training on :(0, 1)\n",
      "training cost:0.0137033909559 and training accuracy:0.997136890888\n",
      "validation cost:0.0140941208228 and validation accuracy:0.996488153934\n",
      "Training on :(0, 1)\n",
      "training cost:0.0115019194782 and training accuracy:0.997657477856\n",
      "validation cost:0.0115101812407 and validation accuracy:0.997366130352\n",
      "Training on :(0, 1)\n",
      "training cost:0.0099402749911 and training accuracy:0.997744202614\n",
      "validation cost:0.00971961114556 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "training cost:0.00863864738494 and training accuracy:0.998004496098\n",
      "validation cost:0.0086479568854 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "training cost:0.00776934204623 and training accuracy:0.99809128046\n",
      "validation cost:0.0084388460964 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "training cost:0.00688597885892 and training accuracy:0.998178005219\n",
      "validation cost:0.00751441437751 and validation accuracy:0.998244047165\n",
      "Training on :(0, 1)\n",
      "Time taken:130.591917992\n",
      "Current mask:[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Test features:(2042, 3630)\n",
      "Length of test labels:2042\n",
      "Train features:(11001, 3630)\n",
      "Length of train labels:11001\n",
      "Valid features:(1088, 3630)\n",
      "Length of valid labels:1088\n",
      "Repeat:0\n",
      "training cost:2.30258536339 and training accuracy:0.0\n",
      "validation cost:2.30258536339 and validation accuracy:0.0\n",
      "Training on :(2, 3)\n",
      "training cost:0.168302044272 and training accuracy:0.950004518032\n",
      "validation cost:0.169079318643 and validation accuracy:0.943014681339\n",
      "Training on :(2, 3)\n",
      "training cost:0.113371483982 and training accuracy:0.968548297882\n",
      "validation cost:0.12098506093 and validation accuracy:0.962316155434\n",
      "Training on :(2, 3)\n"
     ]
    }
   ],
   "source": [
    "method3_test_accs = []\n",
    "#INITIALIZE THE NETWORK\n",
    "logging_count = 0\n",
    "sess.run(init_op,options=run_options, run_metadata=run_metadata)\n",
    "sess.graph.finalize()\n",
    "for l in range(len(lmbdas)):\n",
    "    print('Training with lmbda:{}, {}'.format(lmbdas[l], l))\n",
    "    #sess.run(init_op,options=run_options, run_metadata=run_metadata)\n",
    "    zeta = 1e-3\n",
    "    new_big_omeg_w2 = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "    new_big_omeg_b2 = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "    w3_zeros = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "    b3_zeros = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "    w3_accum = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "    w3_accum = []\n",
    "    b3_accum = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "    b3_accum = []\n",
    "    reset_w2_grad_accum = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "    reset_b2_grad_accum = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "\n",
    "    start_w2 = None\n",
    "    start_b2 = None\n",
    "    sess.run(w_2_update_op, {w_2_update_placeholder:np_weights[l]})\n",
    "    sess.run(b_2_update_op, {b_2_update_placeholder:np.zeros(shape=[1,n_middle],dtype=np.float32)})\n",
    "    end_w2 = None\n",
    "    end_b2 = None\n",
    "\n",
    "    old_test_data = []\n",
    "    historical_cross_test_acc = {}\n",
    "    historical_train_accuracies = {}\n",
    "    historical_train_costs = {}\n",
    "    historical_val_accuracies = {}\n",
    "    historical_val_costs = {}\n",
    "    sets = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
    "    #sets = [(0,4),(5,9)]\n",
    "    test_labels_set = []\n",
    "    \n",
    "    n_test_samples = []\n",
    "    for a_set in range(len(sets)):\n",
    "        current_set = sets[a_set]\n",
    "        current_set_name = 'set'+str(a_set)\n",
    "        mask_val = [0]*num_classes\n",
    "        for i in range(0, current_set[1]+1):\n",
    "            mask_val[i]=1\n",
    "        set_mask_val = np.array(mask_val, dtype=np.float32)\n",
    "        print('Current mask:{}'.format(set_mask_val))\n",
    "        train_data_set, valid_data_set, test_data_set = extract_class_data(start=current_set[0],\n",
    "                                                                      stop=current_set[1])\n",
    "        train_images_set, train_labels_set = train_data_set[0], train_data_set[1]\n",
    "        valid_images_set, valid_labels_set = valid_data_set[0], valid_data_set[1]\n",
    "        test_images_set, test_labels_set = test_data_set[0], test_data_set[1]\n",
    "        n_test_samples.append(len(test_labels_set))\n",
    "        train_total = len(train_images_set)\n",
    "        n_batches = len(train_images_set)/BATCH_SIZE\n",
    "        #@@print('Number of batches:{}'.format(n_batches))\n",
    "\n",
    "\n",
    "        #@@set_omegas = [tf.assign(big_omeg_w2, new_big_omeg_w2), tf.assign(big_omeg_b2, new_big_omeg_b2)]\n",
    "        #@@sess.run(set_omegas)\n",
    "        \n",
    "        sess.run(big_omeg_w2_update_op, {big_omeg_w2_update_placeholder:new_big_omeg_w2})\n",
    "        sess.run(big_omeg_b2_update_op, {big_omeg_b2_update_placeholder:new_big_omeg_b2})\n",
    "\n",
    "        #@@reset_grad_accums = [tf.assign(w2_grad_accum, reset_w2_grad_accum),\n",
    "        #@@                     tf.assign(b2_grad_accum, reset_b2_grad_accum)]\n",
    "        #@@sess.run(reset_grad_accums)\n",
    "        \n",
    "        sess.run(w2_grad_accum_update_op, {w2_grad_accum_update_placeholder:reset_w2_grad_accum})\n",
    "        sess.run(b2_grad_accum_update_op, {b2_grad_accum_update_placeholder:reset_b2_grad_accum})\n",
    "\n",
    "        \n",
    "        #@@reset_w3 = [tf.assign(w_3, w3_zeros), tf.assign(b_3, b3_zeros)]\n",
    "        #@@sess.run(reset_w3)\n",
    "        sess.run(w_3_update_op, {w_3_update_placeholder:w3_zeros})\n",
    "        sess.run(b_3_update_op, {b_3_update_placeholder:b3_zeros})\n",
    "    \n",
    "        epochs = 10\n",
    "        repeats = 1\n",
    "\n",
    "        for repeat in range(repeats):\n",
    "            #tf.set_random_seed(l)\n",
    "            print('Repeat:{}'.format(repeat))\n",
    "            train_accuracies = []\n",
    "            train_costs = []\n",
    "            val_accuracies = []\n",
    "            val_costs = []\n",
    "            best_val = 0\n",
    "            first_params_set = None\n",
    "            last_params_set = None\n",
    "            T1 = time.time()\n",
    "            for i in range(epochs):\n",
    "                if(i==0):\n",
    "                    start_w2, start_b2 = w_2.eval(), b_2.eval()\n",
    "                sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                      batch_size: len(train_images_set)})\n",
    "                #@@print('Epoch:{}'.format((i)))\n",
    "                t1 = time.time()\n",
    "\n",
    "                ### CALCULATE TRAIN COSTS AND TRAIN ACCURACIES\n",
    "                train_cost, train_accuracy = sess.run([cost, acct_res] ,feed_dict = {drop_out : 0.0, \n",
    "                                                                                     set1_mask:set_mask_val})\n",
    "                train_costs.append(train_cost)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "                #train_writer.add_summary(summary,logging_count)\n",
    "\n",
    "                print('training cost:{} and training accuracy:{}'.format(train_costs[i], train_accuracies[i]))\n",
    "\n",
    "                ### CALCULATE VALID COSTS AND VALID ACCURACIES\n",
    "                sess.run(iter.initializer, feed_dict={a_1: valid_images_set, y: valid_labels_set,\n",
    "                                                      batch_size: len(valid_images_set)})\n",
    "                _, _, val_acc, val_cost, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                      feed_dict = {drop_out : 0.0,set1_mask:set_mask_val})\n",
    "                val_costs.append(val_cost)\n",
    "                val_accuracies.append(val_acc)\n",
    "\n",
    "                if(val_acc>best_val):\n",
    "                    best_val = val_acc\n",
    "                    best_params_set1 = [(w_2.eval(),b_2.eval()),(w_3.eval(),b_3.eval())]\n",
    "                print('validation cost:{} and validation accuracy:{}'.format(val_cost, val_acc))   \n",
    "                sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                      batch_size: BATCH_SIZE})\n",
    "\n",
    "\n",
    "                sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                  batch_size: BATCH_SIZE})\n",
    "                print('Training on :{}'.format(current_set))\n",
    "                for j in range(n_batches):\n",
    "\n",
    "                    if(not (np.isnan(w_2.eval().any() and np.isnan(w_3.eval()).any()))):\n",
    "                        #if(a_set==1):\n",
    "                        #    print(j, w_2.eval().sum(), w_3.eval().sum())\n",
    "                        if(((j)% 1000 ==0)):\n",
    "                            logging_count+=1\n",
    "                            summary,_,_ = sess.run([merged,step, omega_step], \n",
    "                                                 feed_dict = {drop_out:0.0,batch_size: BATCH_SIZE,tau:0.5,\n",
    "                                                              set1_mask:set_mask_val,eta:0.001,\n",
    "                                                              lmbda:lmbdas[l],n_tot:train_total})\n",
    "                            #train_writer.add_summary(summary, (i+1)*j)\n",
    "                            train_writer.add_summary(summary, logging_count)\n",
    "                        else:\n",
    "                            sess.run([step, omega_step],feed_dict = {drop_out:0.0,batch_size:BATCH_SIZE,\n",
    "                                                                     tau:0.5,set1_mask:set_mask_val,\n",
    "                                                                     eta:0.001,lmbda:lmbdas[l],\n",
    "                                                                     n_tot:train_total})\n",
    "                    else:\n",
    "                        print('Nan encountered in epoch:{} and batch:{}'.format(i, j))\n",
    "                #@@print('Epoch time:{}'.format(time.time()-t1))\n",
    "\n",
    "\n",
    "            sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                      batch_size: len(test_images_set)})\n",
    "            _,final_test_acc,_ = sess.run([predictions, acct_res, a_3], \n",
    "                                                                  feed_dict = {drop_out:0.0, \n",
    "                                                                               set1_mask:set_mask_val})\n",
    "            #@@print('Final test accuracy is:{}'.format(final_test_acc))\n",
    "            end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "            #@@update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2)]\n",
    "            #@@sess.run(update_star_wbs)\n",
    "            sess.run(star_w2_update_op, {star_w2_update_placeholder:end_w2})\n",
    "            sess.run(star_b2_update_op, {star_b2_update_placeholder:end_b2})\n",
    "            \n",
    "            \n",
    "            #all_final_test_accs_set1.append(final_test_acc)\n",
    "\n",
    "\n",
    "            #@@best_step = [tf.assign(w_2,best_params_set1[0][0]), tf.assign(b_2,best_params_set1[0][1]),\n",
    "            #@@             tf.assign(w_3,best_params_set1[1][0]),tf.assign(b_3,best_params_set1[1][1])]\n",
    "            #@@sess.run(best_step)\n",
    "            \n",
    "            sess.run(w_2_update_op, {w_2_update_placeholder:best_params_set1[0][0]})\n",
    "            sess.run(b_2_update_op, {b_2_update_placeholder:best_params_set1[0][1]})\n",
    "            \n",
    "            sess.run(w_3_update_op, {w_3_update_placeholder:best_params_set1[1][0]})\n",
    "            sess.run(b_3_update_op, {b_3_update_placeholder:best_params_set1[1][1]})\n",
    "            \n",
    "            sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                      batch_size: len(test_images_set)})\n",
    "            _,test_acc_corresp_best_val,_ = sess.run([predictions, acct_res, a_3],\n",
    "                                                     feed_dict = {drop_out:0.0,set1_mask:set_mask_val})\n",
    "\n",
    "            #@@print('Test accuracy corresp to best val acc:{}'.format(test_acc_corresp_best_val))\n",
    "            print('Time taken:{}'.format(time.time()-T1))\n",
    "            #w3_list.append(w_3.eval())\n",
    "            w3_accum.append(w_3.eval())\n",
    "            #b3_list.append(b_3.eval())\n",
    "            b3_accum.append(b_3.eval())\n",
    "            if(i==epochs-1):\n",
    "                if(test_acc_corresp_best_val>final_test_acc):\n",
    "                    end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "                    #all_final_test_accs_set1[-1] = test_acc_corresp_best_val\n",
    "                    #@@update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2)]\n",
    "                    #@@sess.run(update_star_wbs)\n",
    "                    \n",
    "                    sess.run(star_w2_update_op, {star_w2_update_placeholder:end_w2})\n",
    "                    sess.run(star_b2_update_op, {star_b2_update_placeholder:end_b2})\n",
    "                    \n",
    "\n",
    "                #@@best_step = [tf.assign(w_2,end_w2), tf.assign(b_2,end_b2),\n",
    "                #@@         tf.assign(w_3,end_w3),tf.assign(b_3,end_b3)]\n",
    "                #@@sess.run(best_step)\n",
    "                \n",
    "                \n",
    "                sess.run(w_2_update_op, {w_2_update_placeholder:end_w2})\n",
    "                sess.run(b_2_update_op, {b_2_update_placeholder:end_b2})\n",
    "                \n",
    "                sess.run(w_3_update_op, {w_3_update_placeholder:end_w3})\n",
    "                sess.run(b_3_update_op, {b_3_update_placeholder:end_b3})\n",
    "\n",
    "                first_params_set = [(start_w2, start_b2)]\n",
    "                last_params_set = [(end_w2, end_b2)]\n",
    "\n",
    "                small_omegas = [(w2_grad_accum.eval(),b2_grad_accum.eval())]\n",
    "\n",
    "                delta_ws = map(lambda x,y: np.square(x-y)+zeta,[item[0] for item in last_params_set],\n",
    "                           [item[0] for item in first_params_set])\n",
    "\n",
    "                delta_bs = map(lambda x,y: np.square(x-y)+zeta,[item[1] for item in last_params_set],\n",
    "                           [item[1] for item in first_params_set])\n",
    "                delta_wbs = zip(delta_ws, delta_bs)\n",
    "\n",
    "                big_omegas_ws = map(lambda x,y: (x/y),[item[0] for item in small_omegas],\n",
    "                           [item[0] for item in delta_wbs])            \n",
    "                big_omegas_bs = map(lambda x,y: (x/y),[item[1] for item in small_omegas],\n",
    "                           [item[1] for item in delta_wbs])\n",
    "\n",
    "                big_omegas = zip(big_omegas_ws, big_omegas_bs)\n",
    "                new_big_omeg_w2 += big_omegas[0][0]\n",
    "                new_big_omeg_b2 += big_omegas[0][1]\n",
    "                #@@print('omegW2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_w2.max(),\n",
    "                #@@                                                new_big_omeg_w2.mean(),\n",
    "                #@@                                                new_big_omeg_w2.std()))\n",
    "                #@@print('omegb2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_b2.max(),\n",
    "                #@@                                                new_big_omeg_b2.mean(),\n",
    "                #@@                                                new_big_omeg_b2.std()))\n",
    "\n",
    "        historical_train_accuracies[current_set_name]=train_accuracies\n",
    "        historical_train_costs[current_set_name]=train_costs\n",
    "        historical_val_accuracies[current_set_name]=val_accuracies\n",
    "        historical_val_costs[current_set_name]=val_costs\n",
    "    \n",
    "    #######Method 3 ###########\n",
    "    w3_set_rows = []\n",
    "    offset=0\n",
    "    for item in w3_accum:\n",
    "        w3_set_rows.append(item[:,offset:offset+2])\n",
    "        offset+=2\n",
    "\n",
    "\n",
    "    w3_set_row_avgs = [item.mean() for item in w3_set_rows]\n",
    "    final_w3 = map(lambda x,y:x-y,w3_set_rows, w3_set_row_avgs)\n",
    "    final_w3 = np.concatenate(final_w3, axis=1)\n",
    "\n",
    "    b3_set_rows = []\n",
    "    offset=0\n",
    "    for item in b3_accum:\n",
    "        b3_set_rows.append(item[:,offset:offset+2])\n",
    "        offset+=2\n",
    "    b3_set_row_avgs = [item.mean() for item in b3_set_rows]\n",
    "    final_b3 = map(lambda x,y:x-y,b3_set_rows, b3_set_row_avgs)\n",
    "    final_b3 = np.concatenate(final_b3, axis=1)\n",
    "\n",
    "    #@@set_w3 = [tf.assign(w_3, final_w3), tf.assign(b_3, final_b3)]\n",
    "    #@@sess.run(set_w3)\n",
    "    sess.run(w_3_update_op, {w_3_update_placeholder:final_w3})\n",
    "    sess.run(b_3_update_op, {b_3_update_placeholder:final_b3})\n",
    "\n",
    "    sess.run(iter.initializer, feed_dict={a_1: test_images, y: to_categorical(test_labels,num_classes=num_classes),\n",
    "                                                      batch_size: len(test_images)})\n",
    "    _,final_test_acc,_ = sess.run([predictions, acct_res, a_3],\n",
    "                                  feed_dict = {drop_out:0.0,set1_mask:set_mask_val})\n",
    "    method3_test_accs.append(final_test_acc)\n",
    "    print('Method 3 test accuracy:{}'.format(final_test_acc))\n",
    "    \n",
    "    \n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method3_test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_acc = pd.DataFrame({'Method3':method3_test_accs, 'lambdas':lmbdas})\n",
    "#final_test_acc.to_csv('sh_16lmbdas_ar1_3lyrs_he1.csv')\n",
    "final_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_test_accuracies \n",
    "[0.682, 0.7121, 0.7243, 0.5746, 0.5998, 0.6755, 0.7068, 0.7382, 0.6611, 0.6537]\n",
    "for lmbdas = np.array([1.0+i/10.0 for i in range(1,11)])*1.0e5\n",
    "lmbdas = lmbdas.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lmbdas, final_test_accuracies)\n",
    "plt.ticklabel_format(axis='x', style='sci')\n",
    "plt.grid()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_w2.flatten(),bins=100,log=True)\n",
    "#plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_b2.flatten(),100,log=True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_3.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_3.eval().flatten(), 10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the writer without AR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ruthvik/Desktop/Summer 2017/tf_graph_outputs/mnist/continual_learning/original_mnist_5sets'\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(path + '/sh_spike_without_AR1_3lyrs_he1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbdas = [0.0]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_test_accuracies = []\n",
    "#INITIALIZE THE NETWORK\n",
    "for l in range(len(lmbdas)):\n",
    "    print('Training with lmbda:{}'.format(lmbdas[l]))\n",
    "    sess.run(init_op,options=run_options, run_metadata=run_metadata)\n",
    "    zeta = 1e-3\n",
    "    new_big_omeg_w2 = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "    new_big_omeg_b2 = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "    w3_zeros = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "    b3_zeros = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "    w3_accum = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "    b3_accum = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "    reset_w2_grad_accum = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "    reset_b2_grad_accum = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "\n",
    "    start_w2 = None\n",
    "    start_b2 = None\n",
    "\n",
    "    end_w2 = None\n",
    "    end_b2 = None\n",
    "\n",
    "    old_test_data = []\n",
    "    historical_cross_test_acc = {}\n",
    "    historical_train_accuracies = {}\n",
    "    historical_train_costs = {}\n",
    "    historical_val_accuracies = {}\n",
    "    historical_val_costs = {}\n",
    "    sets = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
    "    #sets = [(0,4),(5,9)]\n",
    "    test_labels_set = []\n",
    "    n_test_samples = []\n",
    "    logging_count = 0\n",
    "    for a_set in range(len(sets)):\n",
    "        current_set = sets[a_set]\n",
    "        current_set_name = 'set'+str(a_set)\n",
    "        mask_val = [0]*num_classes\n",
    "        for i in range(0, current_set[1]+1):\n",
    "            mask_val[i]=1\n",
    "        set_mask_val = np.array(mask_val, dtype=np.float32)\n",
    "        print('Current mask:{}'.format(set_mask_val))\n",
    "        train_data_set, valid_data_set, test_data_set = extract_class_data(start=current_set[0],\n",
    "                                                                      stop=current_set[1])\n",
    "        train_images_set, train_labels_set = train_data_set[0], train_data_set[1]\n",
    "        valid_images_set, valid_labels_set = valid_data_set[0], valid_data_set[1]\n",
    "        test_images_set, test_labels_set = test_data_set[0], test_data_set[1]\n",
    "        n_test_samples.append(len(test_labels_set))\n",
    "        train_total = len(train_images_set)\n",
    "        n_batches = len(train_images_set)/BATCH_SIZE\n",
    "        #@@print('Number of batches:{}'.format(n_batches))\n",
    "\n",
    "\n",
    "        set_omegas = [tf.assign(big_omeg_w2, new_big_omeg_w2), tf.assign(big_omeg_b2, new_big_omeg_b2)]\n",
    "        sess.run(set_omegas)\n",
    "\n",
    "        reset_grad_accums = [tf.assign(w2_grad_accum, reset_w2_grad_accum),\n",
    "                             tf.assign(b2_grad_accum, reset_b2_grad_accum)]\n",
    "        sess.run(reset_grad_accums)\n",
    "\n",
    "        reset_w3 = [tf.assign(w_3, w3_zeros), tf.assign(b_3, b3_zeros)]\n",
    "        sess.run(reset_w3)\n",
    "\n",
    "        epochs = 10\n",
    "        repeats = 1\n",
    "\n",
    "        for repeat in range(repeats):\n",
    "            tf.set_random_seed(repeat)\n",
    "            #@@print('Repeat:{}'.format(repeat))\n",
    "            train_accuracies = []\n",
    "            train_costs = []\n",
    "            val_accuracies = []\n",
    "            val_costs = []\n",
    "            best_val = 0\n",
    "            first_params_set = None\n",
    "            last_params_set = None\n",
    "            T1 = time.time()\n",
    "            for i in range(epochs):\n",
    "                if(i==0):\n",
    "                    start_w2, start_b2 = w_2.eval(), b_2.eval()\n",
    "                sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                      batch_size: len(train_images_set)})\n",
    "                #@@print('Epoch:{}'.format((i)))\n",
    "                t1 = time.time()\n",
    "\n",
    "                ### CALCULATE TRAIN COSTS AND TRAIN ACCURACIES\n",
    "                train_cost, train_accuracy = sess.run([cost, acct_res] ,feed_dict = {drop_out : 0.0, \n",
    "                                                                                     set1_mask:set_mask_val})\n",
    "                train_costs.append(train_cost)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "                #train_writer.add_summary(summary,logging_count)\n",
    "\n",
    "                print('training cost:{} and training accuracy:{}'.format(train_costs[i], train_accuracies[i]))\n",
    "\n",
    "                ### CALCULATE VALID COSTS AND VALID ACCURACIES\n",
    "                sess.run(iter.initializer, feed_dict={a_1: valid_images_set, y: valid_labels_set,\n",
    "                                                      batch_size: len(valid_images_set)})\n",
    "                _, _, val_acc, val_cost, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                      feed_dict = {drop_out : 0.0,set1_mask:set_mask_val})\n",
    "                val_costs.append(val_cost)\n",
    "                val_accuracies.append(val_acc)\n",
    "\n",
    "                if(val_acc>best_val):\n",
    "                    best_val = val_acc\n",
    "                    best_params_set1 = [(w_2.eval(),b_2.eval()),(w_3.eval(),b_3.eval())]\n",
    "                #@@print('validation cost:{} and validation accuracy:{}'.format(val_cost, val_acc))   \n",
    "                sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                      batch_size: BATCH_SIZE})\n",
    "\n",
    "\n",
    "                sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                  batch_size: BATCH_SIZE})\n",
    "                print('Training on :{}'.format(current_set))\n",
    "                for j in range(n_batches):\n",
    "\n",
    "                    if(not (np.isnan(w_2.eval().any() and np.isnan(w_3.eval()).any()))):\n",
    "                        #if(a_set==1):\n",
    "                        #    print(j, w_2.eval().sum(), w_3.eval().sum())\n",
    "                        if(((j)% 1000 ==0)):\n",
    "                            logging_count+=1\n",
    "                            summary,_,_ = sess.run([merged,step, omega_step], \n",
    "                                                 feed_dict = {drop_out:0.0,batch_size: BATCH_SIZE,tau:0.5,\n",
    "                                                              set1_mask:set_mask_val,eta:0.001,\n",
    "                                                              lmbda:lmbdas[l],n_tot:train_total})\n",
    "                            #train_writer.add_summary(summary, (i+1)*j)\n",
    "                            train_writer.add_summary(summary, logging_count)\n",
    "                        else:\n",
    "                            sess.run([step, omega_step],feed_dict = {drop_out:0.0,batch_size:BATCH_SIZE,\n",
    "                                                                     tau:0.5,set1_mask:set_mask_val,\n",
    "                                                                     eta:0.001,lmbda:lmbdas[l],\n",
    "                                                                     n_tot:train_total})\n",
    "                    else:\n",
    "                        print('Nan encountered in epoch:{} and batch:{}'.format(i, j))\n",
    "                #@@print('Epoch time:{}'.format(time.time()-t1))\n",
    "\n",
    "\n",
    "            sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                      batch_size: len(test_images_set)})\n",
    "            _,final_test_acc,_ = sess.run([predictions, acct_res, a_3], \n",
    "                                                                  feed_dict = {drop_out:0.0, \n",
    "                                                                               set1_mask:set_mask_val})\n",
    "            #@@print('Final test accuracy is:{}'.format(final_test_acc))\n",
    "            end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "            update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2)]\n",
    "            sess.run(update_star_wbs)\n",
    "            #all_final_test_accs_set1.append(final_test_acc)\n",
    "\n",
    "\n",
    "            best_step = [tf.assign(w_2,best_params_set1[0][0]), tf.assign(b_2,best_params_set1[0][1]),\n",
    "                         tf.assign(w_3,best_params_set1[1][0]),tf.assign(b_3,best_params_set1[1][1])]\n",
    "            sess.run(best_step)\n",
    "            sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                      batch_size: len(test_images_set)})\n",
    "            _,test_acc_corresp_best_val,_ = sess.run([predictions, acct_res, a_3],\n",
    "                                                     feed_dict = {drop_out:0.0,set1_mask:set_mask_val})\n",
    "\n",
    "            #@@print('Test accuracy corresp to best val acc:{}'.format(test_acc_corresp_best_val))\n",
    "            print('Time taken:{}'.format(time.time()-T1))\n",
    "            #w3_list.append(w_3.eval())\n",
    "            w3_accum+=w_3.eval()\n",
    "            #b3_list.append(b_3.eval())\n",
    "            b3_accum+=b_3.eval()\n",
    "            if(i==epochs-1):\n",
    "                if(test_acc_corresp_best_val>final_test_acc):\n",
    "                    end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "                    #all_final_test_accs_set1[-1] = test_acc_corresp_best_val\n",
    "                    update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2)]\n",
    "                    sess.run(update_star_wbs)\n",
    "\n",
    "                best_step = [tf.assign(w_2,end_w2), tf.assign(b_2,end_b2),\n",
    "                         tf.assign(w_3,end_w3),tf.assign(b_3,end_b3)]\n",
    "                sess.run(best_step)\n",
    "\n",
    "                first_params_set = [(start_w2, start_b2)]\n",
    "                last_params_set = [(end_w2, end_b2)]\n",
    "\n",
    "                small_omegas = [(w2_grad_accum.eval(),b2_grad_accum.eval())]\n",
    "\n",
    "                delta_ws = map(lambda x,y: np.square(x-y)+zeta,[item[0] for item in last_params_set],\n",
    "                           [item[0] for item in first_params_set])\n",
    "\n",
    "                delta_bs = map(lambda x,y: np.square(x-y)+zeta,[item[1] for item in last_params_set],\n",
    "                           [item[1] for item in first_params_set])\n",
    "                delta_wbs = zip(delta_ws, delta_bs)\n",
    "\n",
    "                big_omegas_ws = map(lambda x,y: (x/y),[item[0] for item in small_omegas],\n",
    "                           [item[0] for item in delta_wbs])            \n",
    "                big_omegas_bs = map(lambda x,y: (x/y),[item[1] for item in small_omegas],\n",
    "                           [item[1] for item in delta_wbs])\n",
    "\n",
    "                big_omegas = zip(big_omegas_ws, big_omegas_bs)\n",
    "                new_big_omeg_w2 += big_omegas[0][0]\n",
    "                new_big_omeg_b2 += big_omegas[0][1]\n",
    "                #@@print('omegW2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_w2.max(),\n",
    "                #@@                                                new_big_omeg_w2.mean(),\n",
    "                #@@                                                new_big_omeg_w2.std()))\n",
    "                #@@print('omegb2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_b2.max(),\n",
    "                #@@                                                new_big_omeg_b2.mean(),\n",
    "                #@@                                                new_big_omeg_b2.std()))\n",
    "\n",
    "        historical_train_accuracies[current_set_name]=train_accuracies\n",
    "        historical_train_costs[current_set_name]=train_costs\n",
    "        historical_val_accuracies[current_set_name]=val_accuracies\n",
    "        historical_val_costs[current_set_name]=val_costs\n",
    "\n",
    "    avg_w3 = w3_accum/len(sets)\n",
    "    avg_b3 = b3_accum/len(sets)\n",
    "    final_w3 = w3_accum - avg_w3\n",
    "    final_b3 = b3_accum - avg_b3\n",
    "    set_w3 = [tf.assign(w_3, final_w3), tf.assign(b_3, final_b3)]\n",
    "    sess.run(set_w3)\n",
    "\n",
    "    sess.run(iter.initializer, feed_dict={a_1: test_images, y: to_categorical(test_labels,num_classes=num_classes),\n",
    "                                                      batch_size: len(test_images)})\n",
    "    _,final_test_acc,_ = sess.run([predictions, acct_res, a_3],\n",
    "                                  feed_dict = {drop_out:0.0,set1_mask:set_mask_val})\n",
    "    final_test_accuracies.append(final_test_acc)\n",
    "    print('Fina test accuracy:{}'.format(final_test_acc))\n",
    "\n",
    "    \n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_w2.flatten(),bins=100,log=True)\n",
    "#plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_b2.flatten(),100,log=True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_3.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_3.eval().flatten(), 10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* This notebook has implementations of `Backprop` with binary spike feature vectors obtained from the `SPIKEFLOW`. Here, we will use tf.data API. Here the shape of the inputs is `[None, n_input]` instead of `[n_input, None]`. We had to do this because nested elements in `from_tensor_slices` must have the same dimension in 0th rank [see](https://stackoverflow.com/questions/49579684/what-is-the-difference-between-dataset-from-tensors-and-dataset-from-tensor-slic). Everytime an iterator `iter = dataset.make_initializable_iterator()` gets initialized, the dataset is randomly shuffled so we need not shuffle again, [see](https://stackoverflow.com/questions/49579684/what-is-the-difference-between-dataset-from-tensors-and-dataset-from-tensor-slic). We also use `z_3 = tf.floor(z_3)`. Surrogate gradients with one step is used. (one sided)\n",
    "\n",
    "* Here, error in the hidden layer, $\\delta^{2}$ is implemented as:\n",
    "$ \\delta^{2} = W^{3T}\\delta^{(3)}\\odot\\sigma^{'}(z^{(2)}) \\tag{1}$\n",
    "* $\\sigma^{'}(z^{(2)})$ is approximated with a surrogate. (See section 6)\n",
    "* It also takes care of catastrophic forgetting by using synaptic intelligence.\n",
    "## References\n",
    "* [Neural Nets](http://neuralnetworksanddeeplearning.com/chap3.html)\n",
    "* [Randombackprop](https://github.com/xuexue/randombp/blob/master/randombp.py)\n",
    "* [Randombackprop](https://github.com/sangyi92/feedback_alignment/blob/master/RFA.ipynb)\n",
    "* [Backprop](http://blog.aloni.org/posts/backprop-with-tensorflow/)\n",
    "* [Initializers](https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404)\n",
    "* [Dropout](https://github.com/pinae/TensorFlow-MNIST-example/blob/master/fully-connected.py)\n",
    "* [Softmax](https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits)\n",
    "* [SoftmaxLogits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* [TF memory leaks when  assigning in loop](https://github.com/tensorflow/tensorflow/issues/4151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, HTML\n",
    "tf.compat.v2.random.set_seed(0)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.client import timeline\n",
    "import h5py, pickle\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MNIST_Loader\n",
    "import seaborn as sb\n",
    "import theano, random, sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow' from '/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/__init__.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print(tf.__version__)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hide code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = 3.75,3\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['lines.markersize'] = 10\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpl.rcParams['figure.figsize'] = 15,10\n",
    "mpl.rcParams['axes.titlesize'] = 24\n",
    "mpl.rcParams['axes.labelsize'] = 25\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['lines.markersize'] = 10\n",
    "mpl.rcParams['xtick.labelsize'] = 30\n",
    "mpl.rcParams['ytick.labelsize'] = 22\n",
    "mpl.rcParams['legend.fontsize'] = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and separate set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train features:60000\n",
      "Total test features:10000\n"
     ]
    }
   ],
   "source": [
    "filename = '../../spiking_networks/train_pool1_spike_features_inh_False_conv1maps_30.h5'\n",
    "with h5py.File(filename, 'r') as hf:\n",
    "    emnist_train_images = hf['pool1_spike_features'][:].astype(np.int8)\n",
    "emnist_train_images[np.where(emnist_train_images>=1)] = 1\n",
    "\n",
    "filehandle = open('../../spiking_networks/train_y.pkl','rb')\n",
    "emnist_train_labels = pickle.load(filehandle).astype(np.int).tolist()\n",
    "filehandle.close()\n",
    "emnist_train_labels = np.array(emnist_train_labels)\n",
    "print('Total train features:{}'.format(emnist_train_images.shape[0]))\n",
    "\n",
    "#### LOAD TEST IMAGES AND LABELS\n",
    "filename = '../../spiking_networks/test_pool1_spike_features_inh_False_conv1maps_30.h5'\n",
    "with h5py.File(filename, 'r') as hf:\n",
    "    emnist_test_images = hf['pool1_spike_features'][:].astype(np.int8)\n",
    "emnist_test_images[np.where(emnist_test_images>=1)] = 1\n",
    "print('Total test features:{}'.format(emnist_test_images.shape[0]))\n",
    "\n",
    "filehandle = open('../../spiking_networks/test_y.pkl','rb')\n",
    "emnist_test_labels = pickle.load(filehandle).astype(np.int)\n",
    "filehandle.close()\n",
    "emnist_test_labels = np.array(emnist_test_labels)\n",
    "\n",
    "#### LOAD TRAIN AND VALIDATION DATA AND LABELS\n",
    "train_images = emnist_train_images\n",
    "train_labels = emnist_train_labels\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = emnist_test_images\n",
    "test_labels = emnist_test_labels\n",
    "num_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "#### EXTRACT REQUIRED LOCATIONS OF 0 TO 5 FOR TRAIN DATA\n",
    "def extract_class_data(start=0, stop=1):\n",
    "    set1_locs = np.where((train_labels>=start) & (train_labels<=stop))[0]\n",
    "    train_labels_set1 = to_categorical(train_labels[set1_locs], num_classes=num_classes)\n",
    "    train_images_set1 = train_images[set1_locs,:]\n",
    "    n_images = len(train_images_set1)\n",
    "\n",
    "    #### EXTRACT REQUIRED LOCATIONS OF 0 TO 5 FOR TEST DATA\n",
    "    set1_locs = np.where((test_labels>=start) & (test_labels<=stop))[0]\n",
    "    test_labels_set1 = to_categorical(test_labels[set1_locs], num_classes=num_classes)\n",
    "    test_images_set1 = test_images[set1_locs,:]\n",
    "    print('Test features:{}'.format(test_images_set1.shape))\n",
    "    print('Length of test labels:{}'.format(test_labels_set1.shape[0]))\n",
    "    test_data_set1 = (test_images_set1, test_labels_set1)\n",
    "    \n",
    "\n",
    "\n",
    "    train_images_set1 = train_images_set1[int(0.09*n_images):]\n",
    "    train_labels_set1 = train_labels_set1[int(0.09*n_images):]\n",
    "    print('Train features:{}'.format(train_images_set1.shape))\n",
    "    print('Length of train labels:{}'.format(train_labels_set1.shape[0]))\n",
    "    train_data_set1 = (train_images_set1, train_labels_set1)\n",
    "\n",
    "    valid_labels_set1 = train_labels_set1[0:int(0.09*n_images)]\n",
    "    valid_images_set1 = train_images_set1[0:int(0.09*n_images)]\n",
    "    print('Valid features:{}'.format(valid_images_set1.shape))\n",
    "    print('Length of valid labels:{}'.format(valid_labels_set1.shape[0]))\n",
    "    valid_data_set1 = (valid_images_set1, valid_labels_set1)\n",
    "    \n",
    "    n_train_set1 = train_labels_set1.shape[0]\n",
    "    n_test_set1 = test_labels_set1.shape[0]\n",
    "    n_valid_set1 = valid_labels_set1.shape[0]\n",
    "\n",
    "    return train_data_set1, valid_data_set1, test_data_set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess= tf.InteractiveSession(config=config)\n",
    "run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "run_metadata = tf.RunMetadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tanh(x) = \\frac{(e^{x} – e^{-x})}{(e^{x} + e^{-x})}$\n",
    "\n",
    "$d\\frac{tanh(x)}{dx} = 1 – (tanh(x))^{2}$\n",
    "\n",
    "$\\sigma(x) = \\frac{1.0}{1 + e^{-x}}$\n",
    "\n",
    "$d\\frac{\\sigma(x)}{dx} = \\sigma(x)*(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 3630\n",
    "n_middle = 1024\n",
    "n_out = 10\n",
    "batch_size = tf.placeholder(tf.int64, name='batch_size') \n",
    "a_1 = tf.placeholder(tf.float32, [None, n_input], name = 'Input_batch')\n",
    "y = tf.placeholder(tf.float32, [None, n_out], name = 'output_batch')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((a_1, y))\n",
    "#dataset = dataset.shuffle(buffer_size=len(all_train_labels), reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "drop_out = tf.placeholder(tf.float32)\n",
    "tau = tf.placeholder(tf.float32)\n",
    "set1_mask = tf.placeholder(tf.float32, [10], name='mask')\n",
    "eta = tf.placeholder(tf.float32)\n",
    "n_tot = tf.placeholder(tf.float32)\n",
    "lmbda = tf.placeholder(tf.float32, name='lambda')\n",
    "with tf.name_scope('hid_lyr_w_b'):  ###havier or glorot initialization\n",
    "    low = -4*tf.math.sqrt(6.0/(n_input + n_middle)) # use 4 for sigmoid, 1 for tanh activation \n",
    "    high = 4*tf.math.sqrt(6.0/(n_input + n_middle))\n",
    "    \n",
    "    low = -tf.math.sqrt(2.0/(n_input)) # use 4 for sigmoid, 1 for tanh activation \n",
    "    high = tf.math.sqrt(2.0/(n_input))\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_uniform(shape=[n_input,n_middle],minval=low,maxval=high), name = 'W_2')\n",
    "    #w_2 = tf.Variable(tf.truncated_normal(shape=[n_input,n_middle], stddev=0.01),name = 'W_2')\n",
    "    tf.summary.histogram('w_2', w_2)\n",
    "    b_2 = tf.Variable(tf.zeros([1,n_middle]), name = 'b_2')\n",
    "    tf.summary.histogram('b_2', b_2)\n",
    "    \n",
    "    w2_grad_accum = tf.Variable(np.zeros(shape=[n_input,n_middle], dtype=np.float32), name='w2_grad_accum')\n",
    "    b2_grad_accum = tf.Variable(np.zeros(shape=[1,n_middle], dtype=np.float32), name='b2_grad_accum')\n",
    "    \n",
    "    big_omeg_w2 = tf.Variable(np.zeros(shape=[n_input,n_middle], dtype=np.float32), name='omeg_w2')\n",
    "    tf.summary.histogram('big_omeg_w2', big_omeg_w2)\n",
    "    big_omeg_b2 = tf.Variable(np.zeros(shape=[1,n_middle], dtype=np.float32), name='omeg_b2')\n",
    "    tf.summary.histogram('big_omeg_b2', big_omeg_b2)\n",
    "    \n",
    "    star_w2 = tf.Variable(np.zeros(shape=[n_input,n_middle], dtype=np.float32), name='star_w2')\n",
    "    star_b2 = tf.Variable(np.zeros(shape=[1,n_middle], dtype=np.float32), name='star_b2')\n",
    "with tf.name_scope('op_lyr_w_b'):\n",
    "    \n",
    "    low = -tf.math.sqrt(2.0/(n_middle))\n",
    "    high = tf.math.sqrt(2.0/(n_middle))\n",
    "    w_3 = tf.Variable(tf.random_uniform(shape=[n_middle,10],minval=low,maxval=high), name = 'W_3')\n",
    "    #w_3 = tf.Variable(tf.truncated_normal(shape=[n_middle,n_out], stddev=0.01),name = 'W_3')\n",
    "    tf.summary.histogram('w_3', w_3)\n",
    "    b_3 = tf.Variable(tf.zeros([1,n_out]), name = 'b_3')\n",
    "    tf.summary.histogram('b_3', b_3)\n",
    "    \n",
    "    w3_grad_accum = tf.Variable(np.zeros(shape=[n_middle,n_out], dtype=np.float32), name='w3_grad_accum')\n",
    "    b3_grad_accum = tf.Variable(np.zeros(shape=[1,n_out], dtype=np.float32), name='b3_grad_accum')\n",
    "    \n",
    "    big_omeg_w3 = tf.Variable(np.zeros(shape=[n_middle,n_out], dtype=np.float32), name='omeg_w3')\n",
    "    tf.summary.histogram('big_omeg_w3', big_omeg_w3)\n",
    "    big_omeg_b3 = tf.Variable(np.zeros(shape=[1,n_out], dtype=np.float32), name='omeg_b3')\n",
    "    tf.summary.histogram('big_omeg_b3', big_omeg_b3)\n",
    "    \n",
    "    star_w3 = tf.Variable(np.zeros(shape=[n_middle,n_out], dtype=np.float32), name='star_w3')\n",
    "    star_b3 = tf.Variable(np.zeros(shape=[1,n_out], dtype=np.float32), name='star_b3')\n",
    "\n",
    "def sigma(x):\n",
    "    return tf.math.divide(tf.constant(1.0),\n",
    "                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "def tanh(x):\n",
    "    return tf.math.divide(tf.subtract(tf.exp(x), tf.exp(tf.negative(x))), \n",
    "                          tf.add(tf.exp(x), tf.exp(tf.negative(x))) )\n",
    "\n",
    "def sigmaprime(x):\n",
    "    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))\n",
    "\n",
    "def tanhprime(x):\n",
    "    return tf.subtract(tf.constant(1.0),tf.square(tanh(x)))\n",
    "\n",
    "def spkNeuron(x):\n",
    "    return tf.where(tf.greater_equal(x,0.0), tf.ones_like(x), \n",
    "                        tf.zeros_like(x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return tf.maximum(0.0, x)\n",
    "\n",
    "def ReLUprime(x):\n",
    "    return tf.where(tf.greater_equal(x,0.0), tf.ones_like(x), \n",
    "                        tf.zeros_like(x))\n",
    "\n",
    "def spkPrime1(x):\n",
    "    l1_bound_higher = tf.greater_equal(x,-0/4)\n",
    "    r1_bound_lesser = tf.less_equal(x,tau/4) \n",
    "    grad_one = tf.where(tf.logical_and(l1_bound_higher,r1_bound_lesser), tf.ones_like(x), tf.zeros_like(x))\n",
    "    return grad_one\n",
    "\n",
    "def firstLyrSpks(x):\n",
    "    return tf.where(tf.greater_equal(x,1.0), tf.ones_like(x), \n",
    "                        tf.zeros_like(x))\n",
    "\n",
    "    \n",
    "with tf.name_scope('hid_lyr_acti'):\n",
    "    z_2 = tf.add(tf.matmul(features,w_2,name = 'w_2xa_1'), b_2, name = 'z_2')\n",
    "    locs_to_drop = tf.random.categorical(tf.math.log([[1.0-drop_out, drop_out]]), tf.size(z_2))\n",
    "    locs_to_drop = tf.reshape(locs_to_drop, tf.shape(z_2))\n",
    "    z_2 = tf.where(locs_to_drop>0,-tf.ones_like(z_2),z_2, 'drop_out_app')\n",
    "    tf.summary.histogram('z_2', z_2)\n",
    "    #@a_2 = ReLU(z_2)\n",
    "    a_2 = spkNeuron(z_2)\n",
    "    tf.summary.histogram('a_2', a_2)\n",
    "with tf.name_scope('op_lyr_acti'):\n",
    "    z_3 = tf.add(tf.matmul(a_2,w_3, name = 'w_3xa_2'),b_3, name = 'z_3')\n",
    "    z_3 = tf.floor(z_3)\n",
    "    #z_3 = tf.subtract(tf.reduce_max(z_3),z_3, name = 'inhibition')\n",
    "    tf.summary.histogram('z_3', z_3)\n",
    "    #@a_3  = sigma(z_3) ##UNCOMMENT THIS LINE AND COMMENT ABOVE LINE IF YOU WANT spike SQUISHING\n",
    "    a_3 = tf.cast(tf.nn.softmax(z_3,axis=1), tf.float32)\n",
    "    a_3 = tf.multiply(a_3, set1_mask, name='masking')\n",
    "    tf.summary.histogram('a_3', a_3)\n",
    "    ##COMMENT THE ABOVE LINE AND UNCOMMENT BELOW LINE IF YOU WANT SOFTMAX\n",
    "    #a_3 = tf.nn.softmax(z_3,axis=1) ##AXIS IS VERY IMPORTANT!!! axis=1 INDICATES THE CLASSES AS y IS [None,10]\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum((y*tf.log(a_3) +tf.log(1-a_3)*(1-y)) ,axis=0), name = 'cost_calc') WORKS, USE BELOW\n",
    "with tf.name_scope('cost_calc'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,logits=z_3,axis=1),\n",
    "                          name = 'cost_calc')#WORKS\n",
    "        ##COMMENT BELOW LINES IF YOU WANT quadratic\n",
    "    #@dc_da = tf.multiply(-tf.subtract(labels,a_3, name = 'y_minus_a_3'), mask)\n",
    "    #@cost = tf.reduce_mean(tf.reduce_sum((1/2.0)*tf.square(dc_da),axis=1), name = 'cost_calc')\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "with tf.name_scope('op_lyr_grad'):\n",
    "    #@d_z_3 = tf.multiply(-tf.subtract(labels,a_3, name = 'delta3'), mask, name='masking')\n",
    "    d_z_3 = -tf.subtract(labels,a_3, name = 'delta3')\n",
    "    #d_z_3 = tf.multiply(dc_da,a_3, name = 'delta3')\n",
    "    d_b_3 = tf.expand_dims(tf.reduce_mean(d_z_3, axis=[0]), axis=0)\n",
    "    tf.summary.histogram('d_b_3', d_b_3)\n",
    "    d_w_3 = tf.multiply(1/tf.cast(batch_size, tf.float32),\n",
    "                        tf.matmul(tf.transpose(a_2),d_z_3), \n",
    "                        name='delta_w3')\n",
    "    tf.summary.histogram('d_w_3', d_w_3)\n",
    "    \n",
    "with tf.name_scope('hid_lyr_grad'):\n",
    "    #@d_z_2 = tf.multiply(tf.matmul(d_z_3,tf.transpose(w_3), name = 'w_3Txdelta3'), ReLUprime(z_2),\n",
    "    #@                    name = 'delta2')\n",
    "    d_z_2 = tf.multiply(tf.matmul(d_z_3,tf.transpose(w_3), name = 'w_3Txdelta3'), spkPrime1(z_2),\n",
    "                        name = 'delta2')\n",
    "    #d_z_2 = tf.matmul(d_z_3,tf.transpose(w_3), name = 'delta2')\n",
    "    d_b_2 = tf.expand_dims(tf.reduce_mean(d_z_2, axis=[0]), axis=0)\n",
    "    tf.summary.histogram('d_b_2', d_b_2)\n",
    "    d_w_2 = tf.multiply(1/tf.cast(batch_size, tf.float32),\n",
    "                        tf.matmul(tf.transpose(features),d_z_2), \n",
    "                        name='delta_w2')\n",
    "    tf.summary.histogram('d_w_2', d_w_2)\n",
    "    \n",
    "omega_step=[tf.assign(w2_grad_accum,\n",
    "                      tf.add(w2_grad_accum,tf.multiply(eta*eta*lmbda/n_tot, tf.square(d_w_2))),\n",
    "                     name='update_omeg_w2'),\n",
    "            tf.assign(b2_grad_accum,\n",
    "                      tf.add(b2_grad_accum,tf.multiply(eta*eta*lmbda/n_tot,tf.square(d_b_2))),\n",
    "                     name='update_omeg_b2'),\n",
    "            \n",
    "            tf.assign(w3_grad_accum,\n",
    "                      tf.add(w3_grad_accum,tf.multiply(eta*eta*lmbda/n_tot,tf.square(d_w_3))),\n",
    "                     name='update_omeg_w3'),\n",
    "            tf.assign(b3_grad_accum, \n",
    "                      tf.add(b3_grad_accum,tf.multiply(eta*eta*lmbda/n_tot,tf.square(d_b_3))),\n",
    "                     name='update_omeg_b3')\n",
    "]\n",
    "\n",
    "step = [tf.assign(w_2,\n",
    "                  tf.subtract(w_2, (eta*d_w_2+big_omeg_w2*(w_2-star_w2))),name='update_w_2'),\n",
    "        tf.assign(b_2,\n",
    "                  tf.subtract(b_2,(eta*d_b_2+big_omeg_b2*(b_2-star_b2))),name='update_b_2'),\n",
    "        \n",
    "        tf.assign(w_3,\n",
    "                  tf.subtract(w_3, (eta*d_w_3+big_omeg_w3*(w_3-star_w3))),name='update_w_3'),\n",
    "        tf.assign(b_3,\n",
    "                  tf.subtract(b_3,(eta*d_b_3+big_omeg_b3*(b_3-star_b3))),name='update_b_3')    \n",
    "]\n",
    "with tf.name_scope('acc_calc'):\n",
    "    predictions = tf.argmax(a_3, 1)\n",
    "    acct_mat = tf.equal(tf.argmax(a_3, 1), tf.argmax(labels, 1))\n",
    "    acct_res = tf.reduce_mean(tf.cast(acct_mat, tf.float32))\n",
    "    tf.summary.scalar('accuracy', acct_res)\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the writer with SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ruthvik/Desktop/Summer 2017/tf_graph_outputs/mnist/continual_learning/original_mnist_5sets'\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(path + '/mh_spike_3lyrs_he1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features:(2115, 3630)\n",
      "Length of test labels:2115\n",
      "Train features:(11526, 3630)\n",
      "Length of train labels:11526\n",
      "Valid features:(1139, 3630)\n",
      "Length of valid labels:1139\n",
      "Number of batches:1152\n",
      "Repeat:0\n",
      "Epoch:0\n",
      "training cost:2.18432164192 and training accuracy:0.473104298115\n",
      "validation cost:2.18295145035 and validation accuracy:0.467954337597\n",
      "Training on :(0, 1)\n",
      "Epoch time:45.1382958889\n",
      "Epoch:1\n",
      "training cost:0.017300978303 and training accuracy:0.99514144659\n",
      "validation cost:0.0186073817313 and validation accuracy:0.994732201099\n",
      "Training on :(0, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4367542d87de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m                         sess.run([step, omega_step], feed_dict = {drop_out:0.5,batch_size: BATCH_SIZE, tau:0.5,\n\u001b[1;32m    146\u001b[0m                                                                  \u001b[0mset1_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mset_mask_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                                                                   lmbda:1.0e4,n_tot:train_total})\n\u001b[0m\u001b[1;32m    148\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Nan encountered in epoch:{} and batch:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruthvik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#INITIALIZE THE NETWORK\n",
    "sess.run(init_op,options=run_options, run_metadata=run_metadata)\n",
    "zeta = 1e-3\n",
    "new_big_omeg_w2 = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "new_big_omeg_b2 = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "new_big_omeg_w3 = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "new_big_omeg_b3 = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "\n",
    "reset_w2_grad_accum = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "reset_b2_grad_accum = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "reset_w3_grad_accum = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "reset_b3_grad_accum = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "    \n",
    "start_w2 = None\n",
    "start_b2 = None\n",
    "start_w3 = None\n",
    "start_b3 = None\n",
    "\n",
    "end_w2 = None\n",
    "end_b2 = None\n",
    "end_w3 = None\n",
    "end_b3 = None\n",
    "\n",
    "old_test_data = []\n",
    "historical_cross_test_acc = {}\n",
    "historical_train_accuracies = {}\n",
    "historical_train_costs = {}\n",
    "historical_val_accuracies = {}\n",
    "historical_val_costs = {}\n",
    "sets = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
    "#sets = [(0,4),(5,9)]\n",
    "test_labels_set = []\n",
    "logging_count = 0\n",
    "n_test_samples = []\n",
    "for a_set in range(len(sets)):\n",
    "    current_set = sets[a_set]\n",
    "    current_set_name = 'set'+str(a_set)\n",
    "    mask_val = [0]*num_classes\n",
    "    for i in range(current_set[0], current_set[1]+1):\n",
    "        mask_val[i]=1\n",
    "    set_mask_val = np.array(mask_val, dtype=np.float32)\n",
    "    train_data_set, valid_data_set, test_data_set = extract_class_data(start=current_set[0],\n",
    "                                                                  stop=current_set[1])\n",
    "    train_images_set, train_labels_set = train_data_set[0], train_data_set[1]\n",
    "    valid_images_set, valid_labels_set = valid_data_set[0], valid_data_set[1]\n",
    "    test_images_set, test_labels_set = test_data_set[0], test_data_set[1]\n",
    "    n_test_samples.append(len(test_labels_set))\n",
    "    train_total = len(train_images_set)\n",
    "    n_batches = len(train_images_set)/BATCH_SIZE\n",
    "    print('Number of batches:{}'.format(n_batches))\n",
    "\n",
    "\n",
    "    set_omegas = [tf.assign(big_omeg_w2, new_big_omeg_w2), tf.assign(big_omeg_b2, new_big_omeg_b2), \n",
    "                  tf.assign(big_omeg_w3, new_big_omeg_w3), tf.assign(big_omeg_b3, new_big_omeg_b3)]\n",
    "    sess.run(set_omegas)\n",
    "    \n",
    "    reset_grad_accums = [tf.assign(w2_grad_accum, reset_w2_grad_accum),\n",
    "                         tf.assign(b2_grad_accum, reset_b2_grad_accum),\n",
    "                         tf.assign(w3_grad_accum, reset_w3_grad_accum),\n",
    "                         tf.assign(b3_grad_accum, reset_b3_grad_accum)]\n",
    "    sess.run(reset_grad_accums)\n",
    "                                                                                  \n",
    "    epochs = 60\n",
    "    repeats = 1\n",
    "    \n",
    "    for repeat in range(repeats):\n",
    "        tf.set_random_seed(repeat)\n",
    "        print('Repeat:{}'.format(repeat))\n",
    "        train_accuracies = []\n",
    "        train_costs = []\n",
    "        val_accuracies = []\n",
    "        val_costs = []\n",
    "        best_val = 0\n",
    "        first_params_set = None\n",
    "        last_params_set = None\n",
    "        T1 = time.time()\n",
    "        for i in range(epochs):\n",
    "            if(i==0):\n",
    "                start_w2, start_b2, start_w3, start_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "            sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                  batch_size: len(train_images_set)})\n",
    "            print('Epoch:{}'.format((i)))\n",
    "            t1 = time.time()\n",
    "\n",
    "            ### CALCULATE TRAIN COSTS AND TRAIN ACCURACIES\n",
    "            train_cost, train_accuracy = sess.run([cost, acct_res] ,feed_dict = {drop_out : 0.0, \n",
    "                                                                                 set1_mask:set_mask_val})\n",
    "            train_costs.append(train_cost)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            #train_writer.add_summary(summary,logging_count)\n",
    "\n",
    "            print('training cost:{} and training accuracy:{}'.format(train_costs[i], train_accuracies[i]))\n",
    "\n",
    "            ### CALCULATE VALID COSTS AND VALID ACCURACIES\n",
    "            sess.run(iter.initializer, feed_dict={a_1: valid_images_set, y: valid_labels_set,\n",
    "                                                  batch_size: len(valid_images_set)})\n",
    "            _, _, val_acc, val_cost, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                  feed_dict = {drop_out : 0.0,set1_mask:set_mask_val})\n",
    "            val_costs.append(val_cost)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            if(val_acc>best_val):\n",
    "                best_val = val_acc\n",
    "                best_params_set1 = [(w_2.eval(),b_2.eval()),(w_3.eval(),b_3.eval())]\n",
    "            print('validation cost:{} and validation accuracy:{}'.format(val_cost, val_acc))   \n",
    "            sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                  batch_size: BATCH_SIZE})\n",
    "            \n",
    "            for d in range(len(old_test_data)):\n",
    "                previous_set_name = 'set'+str(d)           \n",
    "                prev_set = sets[d]\n",
    "                prev_mask_val = [0]*num_classes\n",
    "                for clas in range(prev_set[0], prev_set[1]+1):\n",
    "                    prev_mask_val[clas]=1\n",
    "                prev_set_mask_val = np.array(prev_mask_val, dtype=np.float32)\n",
    "                sess.run(iter.initializer, feed_dict={a_1: old_test_data[d][0], y: old_test_data[d][1],\n",
    "                                                  batch_size: len(old_test_data[d][0])})\n",
    "                _, _, hist_test_acc, _, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                      feed_dict = {drop_out : 0.0,set1_mask:prev_set_mask_val})\n",
    "                \n",
    "                print('Testing accuracy on :{} while training :{} is :{}'.format(previous_set_name,\n",
    "                                                                          current_set_name,\n",
    "                                                                          hist_test_acc))\n",
    "                if(current_set_name+'-'+previous_set_name in historical_cross_test_acc.keys()):\n",
    "                    historical_cross_test_acc[current_set_name+'-'+previous_set_name].append(hist_test_acc)\n",
    "                else:\n",
    "                    historical_cross_test_acc[current_set_name+'-'+previous_set_name] = [hist_test_acc]\n",
    "            sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                              batch_size: BATCH_SIZE})\n",
    "            print('Training on :{}'.format(current_set))\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                if(not (np.isnan(w_2.eval().any() and np.isnan(w_3.eval()).any()))):\n",
    "                    #if(a_set==1):\n",
    "                    #    print(j, w_2.eval().sum(), w_3.eval().sum())\n",
    "                    if(((j)% 1000 ==0)):\n",
    "                        logging_count+=1\n",
    "                        summary,_,_ = sess.run([merged,step, omega_step], \n",
    "                                             feed_dict = {drop_out:0.5,batch_size: BATCH_SIZE, tau:0.5,\n",
    "                                                          set1_mask:set_mask_val, eta:0.001,\n",
    "                                                          lmbda:1.0e4,n_tot:train_total})\n",
    "                        #train_writer.add_summary(summary, (i+1)*j)\n",
    "                        train_writer.add_summary(summary, logging_count)\n",
    "                    else:\n",
    "                        sess.run([step, omega_step], feed_dict = {drop_out:0.5,batch_size: BATCH_SIZE, tau:0.5,\n",
    "                                                                 set1_mask:set_mask_val, eta:0.001,\n",
    "                                                                  lmbda:1.0e4,n_tot:train_total})\n",
    "                else:\n",
    "                    print('Nan encountered in epoch:{} and batch:{}'.format(i, j))\n",
    "            print('Epoch time:{}'.format(time.time()-t1))\n",
    "\n",
    "\n",
    "        sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                  batch_size: len(test_images_set)})\n",
    "        _,final_test_acc,_ = sess.run([predictions, acct_res, a_3], \n",
    "                                                              feed_dict = {drop_out:0.0, \n",
    "                                                                           set1_mask:set_mask_val})\n",
    "        print('Final test accuracy is:{}'.format(final_test_acc))\n",
    "        end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "        update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2),tf.assign(star_w3,end_w3),\n",
    "                          tf.assign(star_b3,end_b3)]\n",
    "        sess.run(update_star_wbs)\n",
    "        #all_final_test_accs_set1.append(final_test_acc)\n",
    "\n",
    "\n",
    "        best_step = [tf.assign(w_2,best_params_set1[0][0]), tf.assign(b_2,best_params_set1[0][1]),\n",
    "                     tf.assign(w_3,best_params_set1[1][0]),tf.assign(b_3,best_params_set1[1][1])]\n",
    "        sess.run(best_step)\n",
    "        sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                  batch_size: len(test_images_set)})\n",
    "        _,test_acc_corresp_best_val,_ = sess.run([predictions, acct_res, a_3],\n",
    "                                                 feed_dict = {drop_out:0.0,set1_mask:set_mask_val})\n",
    "\n",
    "        print('Test accuracy corresp to best val acc:{}'.format(test_acc_corresp_best_val))\n",
    "        print('Time taken:{}'.format(time.time()-T1))\n",
    "        if(i==epochs-1):\n",
    "            if(test_acc_corresp_best_val>final_test_acc):\n",
    "                end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "                #all_final_test_accs_set1[-1] = test_acc_corresp_best_val\n",
    "                update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2),tf.assign(star_w3,end_w3),\n",
    "                          tf.assign(star_b3,end_b3)]\n",
    "                sess.run(update_star_wbs)\n",
    "            \n",
    "            best_step = [tf.assign(w_2,end_w2), tf.assign(b_2,end_b2),\n",
    "                     tf.assign(w_3,end_w3),tf.assign(b_3,end_b3)]\n",
    "            sess.run(best_step)\n",
    "            \n",
    "            first_params_set = [(start_w2, start_b2), (start_w3, start_b3)]\n",
    "            last_params_set = [(end_w2, end_b2), (end_w3, end_b3)]\n",
    "            \n",
    "            small_omegas = [(w2_grad_accum.eval(), b2_grad_accum.eval()), (w3_grad_accum.eval(),\n",
    "                           b3_grad_accum.eval())]\n",
    "            \n",
    "            delta_ws = map(lambda x,y: np.square(x-y)+zeta,[item[0] for item in last_params_set],\n",
    "                       [item[0] for item in first_params_set])\n",
    "            \n",
    "            delta_bs = map(lambda x,y: np.square(x-y)+zeta,[item[1] for item in last_params_set],\n",
    "                       [item[1] for item in first_params_set])\n",
    "            delta_wbs = zip(delta_ws, delta_bs)\n",
    "            \n",
    "            big_omegas_ws = map(lambda x,y: (x/y),[item[0] for item in small_omegas],\n",
    "                       [item[0] for item in delta_wbs])\n",
    "            \n",
    "            big_omegas_bs = map(lambda x,y: (x/y),[item[1] for item in small_omegas],\n",
    "                       [item[1] for item in delta_wbs])\n",
    "            \n",
    "            big_omegas = zip(big_omegas_ws, big_omegas_bs)\n",
    "            if(a_set != len(sets)-1):     \n",
    "                new_big_omeg_w2 += big_omegas[0][0]\n",
    "                new_big_omeg_b2 += big_omegas[0][1]\n",
    "                new_big_omeg_w3 += big_omegas[1][0]\n",
    "                new_big_omeg_b3 += big_omegas[1][1]\n",
    "            \n",
    "            for d in range(len(old_test_data)):\n",
    "                previous_set_name = 'set'+str(d)\n",
    "                prev_set = sets[d]\n",
    "                prev_mask_val = [0]*num_classes\n",
    "                for clas in range(prev_set[0], prev_set[1]+1):\n",
    "                    prev_mask_val[clas]=1\n",
    "                prev_set_mask_val = np.array(prev_mask_val, dtype=np.float32)\n",
    "                sess.run(iter.initializer, feed_dict={a_1: old_test_data[d][0], y: old_test_data[d][1],\n",
    "                                                  batch_size: len(old_test_data[d][0])})\n",
    "                _, _, hist_test_acc, _, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                      feed_dict = {drop_out : 0.0,set1_mask:prev_set_mask_val})\n",
    "                \n",
    "                historical_cross_test_acc[current_set_name+'-'+previous_set_name].append(hist_test_acc)\n",
    "                print('Testing accuracy on :{} after training :{} is :{}'.format(previous_set_name,\n",
    "                                                                          current_set_name,\n",
    "                                                                          hist_test_acc))\n",
    "                historical_cross_test_acc[current_set_name+'-'+current_set_name]=[test_acc_corresp_best_val]\n",
    "            old_test_data.append(test_data_set)\n",
    "            print('omegW2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_w2.max(),\n",
    "                                                            new_big_omeg_w2.mean(),\n",
    "                                                            new_big_omeg_w2.std()))\n",
    "            print('omegb2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_b2.max(),\n",
    "                                                            new_big_omeg_b2.mean(),\n",
    "                                                            new_big_omeg_b2.std()))\n",
    "            print('omegW3-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_w3.max(),\n",
    "                                                            new_big_omeg_w3.mean(),\n",
    "                                                            new_big_omeg_w3.std()))\n",
    "            print('omegb3-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_b3.max(),\n",
    "                                                            new_big_omeg_b3.mean(),\n",
    "                                                            new_big_omeg_b3.std()))\n",
    "            #sys.exit()\n",
    "    historical_train_accuracies[current_set_name]=train_accuracies\n",
    "    historical_train_costs[current_set_name]=train_costs\n",
    "    historical_val_accuracies[current_set_name]=val_accuracies\n",
    "    historical_val_costs[current_set_name]=val_costs\n",
    "            \n",
    "            \n",
    "train_writer.close()\n",
    "#valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_accs = []\n",
    "for i in range(0,num_classes/2):\n",
    "    set_acc =  historical_cross_test_acc['set4-set'+str(i)][-1]*100\n",
    "    print('Accuracy on set {}:{} after training set {}:{} is:{}'.format(i, sets[i],\\\n",
    "                                    4, sets[2], set_acc))\n",
    "    set_accs.append(set_acc)\n",
    "n_test_samples = np.array(n_test_samples)\n",
    "final_acc = (n_test_samples*set_accs).sum()/n_test_samples.sum()\n",
    "print('Final accuracy on all sets:{}'.format(final_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_w2.flatten(),bins=100,log=True)\n",
    "#plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_b2.flatten(),100,log=True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_3.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_W_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_w3.flatten(),100,log=True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_3.eval().flatten(), 10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_b3.flatten(),10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the writer without SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ruthvik/Desktop/Summer 2017/tf_graph_outputs/mnist/continual_learning/original_mnist_5sets'\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(path + '/mh_spike_without_SI_3lyrs_trunc1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INITIALIZE THE NETWORK\n",
    "sess.run(init_op,options=run_options, run_metadata=run_metadata)\n",
    "zeta = 1e-3\n",
    "new_big_omeg_w2 = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "new_big_omeg_b2 = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "new_big_omeg_w3 = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "new_big_omeg_b3 = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "\n",
    "reset_w2_grad_accum = np.zeros(shape=[n_input,n_middle], dtype=np.float32)\n",
    "reset_b2_grad_accum = np.zeros(shape=[1,n_middle], dtype=np.float32)\n",
    "reset_w3_grad_accum = np.zeros(shape=[n_middle,n_out], dtype=np.float32)\n",
    "reset_b3_grad_accum = np.zeros(shape=[1,n_out], dtype=np.float32)\n",
    "    \n",
    "start_w2 = None\n",
    "start_b2 = None\n",
    "start_w3 = None\n",
    "start_b3 = None\n",
    "\n",
    "end_w2 = None\n",
    "end_b2 = None\n",
    "end_w3 = None\n",
    "end_b3 = None\n",
    "\n",
    "old_test_data = []\n",
    "historical_cross_test_acc = {}\n",
    "historical_train_accuracies = {}\n",
    "historical_train_costs = {}\n",
    "historical_val_accuracies = {}\n",
    "historical_val_costs = {}\n",
    "sets = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
    "#sets = [(0,4),(5,9)]\n",
    "test_labels_set = []\n",
    "logging_count = 0\n",
    "n_test_samples = []\n",
    "for a_set in range(len(sets)):\n",
    "    current_set = sets[a_set]\n",
    "    current_set_name = 'set'+str(a_set)\n",
    "    mask_val = [0]*num_classes\n",
    "    for i in range(current_set[0], current_set[1]+1):\n",
    "        mask_val[i]=1\n",
    "    set_mask_val = np.array(mask_val, dtype=np.float32)\n",
    "    train_data_set, valid_data_set, test_data_set = extract_class_data(start=current_set[0],\n",
    "                                                                  stop=current_set[1])\n",
    "    train_images_set, train_labels_set = train_data_set[0], train_data_set[1]\n",
    "    valid_images_set, valid_labels_set = valid_data_set[0], valid_data_set[1]\n",
    "    test_images_set, test_labels_set = test_data_set[0], test_data_set[1]\n",
    "    n_test_samples.append(len(test_labels_set))\n",
    "    train_total = len(train_images_set)\n",
    "    n_batches = len(train_images_set)/BATCH_SIZE\n",
    "    print('Number of batches:{}'.format(n_batches))\n",
    "\n",
    "\n",
    "    set_omegas = [tf.assign(big_omeg_w2, new_big_omeg_w2), tf.assign(big_omeg_b2, new_big_omeg_b2), \n",
    "                  tf.assign(big_omeg_w3, new_big_omeg_w3), tf.assign(big_omeg_b3, new_big_omeg_b3)]\n",
    "    sess.run(set_omegas)\n",
    "    \n",
    "    reset_grad_accums = [tf.assign(w2_grad_accum, reset_w2_grad_accum),\n",
    "                         tf.assign(b2_grad_accum, reset_b2_grad_accum),\n",
    "                         tf.assign(w3_grad_accum, reset_w3_grad_accum),\n",
    "                         tf.assign(b3_grad_accum, reset_b3_grad_accum)]\n",
    "    sess.run(reset_grad_accums)\n",
    "                                                                                  \n",
    "    epochs = 60\n",
    "    repeats = 1\n",
    "    \n",
    "    for repeat in range(repeats):\n",
    "        tf.set_random_seed(repeat)\n",
    "        print('Repeat:{}'.format(repeat))\n",
    "        train_accuracies = []\n",
    "        train_costs = []\n",
    "        val_accuracies = []\n",
    "        val_costs = []\n",
    "        best_val = 0\n",
    "        first_params_set = None\n",
    "        last_params_set = None\n",
    "        T1 = time.time()\n",
    "        for i in range(epochs):\n",
    "            if(i==0):\n",
    "                start_w2, start_b2, start_w3, start_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "            sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                  batch_size: len(train_images_set)})\n",
    "            print('Epoch:{}'.format((i)))\n",
    "            t1 = time.time()\n",
    "\n",
    "            ### CALCULATE TRAIN COSTS AND TRAIN ACCURACIES\n",
    "            train_cost, train_accuracy = sess.run([cost, acct_res] ,feed_dict = {drop_out : 0.0, \n",
    "                                                                                 set1_mask:set_mask_val})\n",
    "            train_costs.append(train_cost)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            #train_writer.add_summary(summary,logging_count)\n",
    "\n",
    "            print('training cost:{} and training accuracy:{}'.format(train_costs[i], train_accuracies[i]))\n",
    "\n",
    "            ### CALCULATE VALID COSTS AND VALID ACCURACIES\n",
    "            sess.run(iter.initializer, feed_dict={a_1: valid_images_set, y: valid_labels_set,\n",
    "                                                  batch_size: len(valid_images_set)})\n",
    "            _, _, val_acc, val_cost, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                  feed_dict = {drop_out : 0.0,set1_mask:set_mask_val})\n",
    "            val_costs.append(val_cost)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            if(val_acc>best_val):\n",
    "                best_val = val_acc\n",
    "                best_params_set1 = [(w_2.eval(),b_2.eval()),(w_3.eval(),b_3.eval())]\n",
    "            print('validation cost:{} and validation accuracy:{}'.format(val_cost, val_acc))   \n",
    "            sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                                  batch_size: BATCH_SIZE})\n",
    "            \n",
    "            for d in range(len(old_test_data)):\n",
    "                previous_set_name = 'set'+str(d)           \n",
    "                prev_set = sets[d]\n",
    "                prev_mask_val = [0]*num_classes\n",
    "                for clas in range(prev_set[0], prev_set[1]+1):\n",
    "                    prev_mask_val[clas]=1\n",
    "                prev_set_mask_val = np.array(prev_mask_val, dtype=np.float32)\n",
    "                sess.run(iter.initializer, feed_dict={a_1: old_test_data[d][0], y: old_test_data[d][1],\n",
    "                                                  batch_size: len(old_test_data[d][0])})\n",
    "                _, _, hist_test_acc, _, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                      feed_dict = {drop_out : 0.0,set1_mask:prev_set_mask_val})\n",
    "                \n",
    "                print('Testing accuracy on :{} while training :{} is :{}'.format(previous_set_name,\n",
    "                                                                          current_set_name,\n",
    "                                                                          hist_test_acc))\n",
    "                if(current_set_name+'-'+previous_set_name in historical_cross_test_acc.keys()):\n",
    "                    historical_cross_test_acc[current_set_name+'-'+previous_set_name].append(hist_test_acc)\n",
    "                else:\n",
    "                    historical_cross_test_acc[current_set_name+'-'+previous_set_name] = [hist_test_acc]\n",
    "            sess.run(iter.initializer, feed_dict={a_1: train_images_set, y: train_labels_set,\n",
    "                                              batch_size: BATCH_SIZE})\n",
    "            print('Training on :{}'.format(current_set))\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                if(not (np.isnan(w_2.eval().any() and np.isnan(w_3.eval()).any()))):\n",
    "                    #if(a_set==1):\n",
    "                    #    print(j, w_2.eval().sum(), w_3.eval().sum())\n",
    "                    if(((j)% 1000 ==0)):\n",
    "                        logging_count+=1\n",
    "                        summary,_,_ = sess.run([merged,step, omega_step], \n",
    "                                             feed_dict = {drop_out:0.5,batch_size: BATCH_SIZE, tau:0.5,\n",
    "                                                          set1_mask:set_mask_val, eta:0.001,\n",
    "                                                          lmbda:0.0e4,n_tot:train_total})\n",
    "                        #train_writer.add_summary(summary, (i+1)*j)\n",
    "                        train_writer.add_summary(summary, logging_count)\n",
    "                    else:\n",
    "                        sess.run([step, omega_step], feed_dict = {drop_out:0.5,batch_size: BATCH_SIZE, tau:0.5,\n",
    "                                                                 set1_mask:set_mask_val, eta:0.001,\n",
    "                                                                  lmbda:0.0e4,n_tot:train_total})\n",
    "                else:\n",
    "                    print('Nan encountered in epoch:{} and batch:{}'.format(i, j))\n",
    "            print('Epoch time:{}'.format(time.time()-t1))\n",
    "\n",
    "\n",
    "        sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                  batch_size: len(test_images_set)})\n",
    "        _,final_test_acc,_ = sess.run([predictions, acct_res, a_3], \n",
    "                                                              feed_dict = {drop_out:0.0, \n",
    "                                                                           set1_mask:set_mask_val})\n",
    "        print('Final test accuracy is:{}'.format(final_test_acc))\n",
    "        end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "        update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2),tf.assign(star_w3,end_w3),\n",
    "                          tf.assign(star_b3,end_b3)]\n",
    "        sess.run(update_star_wbs)\n",
    "        #all_final_test_accs_set1.append(final_test_acc)\n",
    "\n",
    "\n",
    "        best_step = [tf.assign(w_2,best_params_set1[0][0]), tf.assign(b_2,best_params_set1[0][1]),\n",
    "                     tf.assign(w_3,best_params_set1[1][0]),tf.assign(b_3,best_params_set1[1][1])]\n",
    "        sess.run(best_step)\n",
    "        sess.run(iter.initializer, feed_dict={a_1: test_images_set, y: test_labels_set,\n",
    "                                                  batch_size: len(test_images_set)})\n",
    "        _,test_acc_corresp_best_val,_ = sess.run([predictions, acct_res, a_3],\n",
    "                                                 feed_dict = {drop_out:0.0,set1_mask:set_mask_val})\n",
    "\n",
    "        print('Test accuracy corresp to best val acc:{}'.format(test_acc_corresp_best_val))\n",
    "        print('Time taken:{}'.format(time.time()-T1))\n",
    "        if(i==epochs-1):\n",
    "            if(test_acc_corresp_best_val>final_test_acc):\n",
    "                end_w2, end_b2, end_w3, end_b3 = w_2.eval(), b_2.eval(), w_3.eval(), b_3.eval()\n",
    "                #all_final_test_accs_set1[-1] = test_acc_corresp_best_val\n",
    "                update_star_wbs = [tf.assign(star_w2,end_w2),tf.assign(star_b2,end_b2),tf.assign(star_w3,end_w3),\n",
    "                          tf.assign(star_b3,end_b3)]\n",
    "                sess.run(update_star_wbs)\n",
    "            \n",
    "            best_step = [tf.assign(w_2,end_w2), tf.assign(b_2,end_b2),\n",
    "                     tf.assign(w_3,end_w3),tf.assign(b_3,end_b3)]\n",
    "            sess.run(best_step)\n",
    "            \n",
    "            first_params_set = [(start_w2, start_b2), (start_w3, start_b3)]\n",
    "            last_params_set = [(end_w2, end_b2), (end_w3, end_b3)]\n",
    "            \n",
    "            small_omegas = [(w2_grad_accum.eval(), b2_grad_accum.eval()), (w3_grad_accum.eval(),\n",
    "                           b3_grad_accum.eval())]\n",
    "            \n",
    "            delta_ws = map(lambda x,y: np.square(x-y)+zeta,[item[0] for item in last_params_set],\n",
    "                       [item[0] for item in first_params_set])\n",
    "            \n",
    "            delta_bs = map(lambda x,y: np.square(x-y)+zeta,[item[1] for item in last_params_set],\n",
    "                       [item[1] for item in first_params_set])\n",
    "            delta_wbs = zip(delta_ws, delta_bs)\n",
    "            \n",
    "            big_omegas_ws = map(lambda x,y: (x/y),[item[0] for item in small_omegas],\n",
    "                       [item[0] for item in delta_wbs])\n",
    "            \n",
    "            big_omegas_bs = map(lambda x,y: (x/y),[item[1] for item in small_omegas],\n",
    "                       [item[1] for item in delta_wbs])\n",
    "            \n",
    "            big_omegas = zip(big_omegas_ws, big_omegas_bs)\n",
    "            if(a_set != len(sets)-1):     \n",
    "                new_big_omeg_w2 += big_omegas[0][0]\n",
    "                new_big_omeg_b2 += big_omegas[0][1]\n",
    "                new_big_omeg_w3 += big_omegas[1][0]\n",
    "                new_big_omeg_b3 += big_omegas[1][1]\n",
    "            \n",
    "            for d in range(len(old_test_data)):\n",
    "                previous_set_name = 'set'+str(d)\n",
    "                prev_set = sets[d]\n",
    "                prev_mask_val = [0]*num_classes\n",
    "                for clas in range(prev_set[0], prev_set[1]+1):\n",
    "                    prev_mask_val[clas]=1\n",
    "                prev_set_mask_val = np.array(prev_mask_val, dtype=np.float32)\n",
    "                sess.run(iter.initializer, feed_dict={a_1: old_test_data[d][0], y: old_test_data[d][1],\n",
    "                                                  batch_size: len(old_test_data[d][0])})\n",
    "                _, _, hist_test_acc, _, _ = sess.run([predictions,acct_mat,acct_res, cost, a_3],\n",
    "                                                      feed_dict = {drop_out : 0.0,set1_mask:prev_set_mask_val})\n",
    "                \n",
    "                historical_cross_test_acc[current_set_name+'-'+previous_set_name].append(hist_test_acc)\n",
    "                print('Testing accuracy on :{} after training :{} is :{}'.format(previous_set_name,\n",
    "                                                                          current_set_name,\n",
    "                                                                          hist_test_acc))\n",
    "                historical_cross_test_acc[current_set_name+'-'+current_set_name]=[test_acc_corresp_best_val]\n",
    "            old_test_data.append(test_data_set)\n",
    "            print('omegW2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_w2.max(),\n",
    "                                                            new_big_omeg_w2.mean(),\n",
    "                                                            new_big_omeg_w2.std()))\n",
    "            print('omegb2-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_b2.max(),\n",
    "                                                            new_big_omeg_b2.mean(),\n",
    "                                                            new_big_omeg_b2.std()))\n",
    "            print('omegW3-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_w3.max(),\n",
    "                                                            new_big_omeg_w3.mean(),\n",
    "                                                            new_big_omeg_w3.std()))\n",
    "            print('omegb3-MAXIMUM:{},MEAN:{},STD:{}'.format(new_big_omeg_b3.max(),\n",
    "                                                            new_big_omeg_b3.mean(),\n",
    "                                                            new_big_omeg_b3.std()))\n",
    "            #sys.exit()\n",
    "    historical_train_accuracies[current_set_name]=train_accuracies\n",
    "    historical_train_costs[current_set_name]=train_costs\n",
    "    historical_val_accuracies[current_set_name]=val_accuracies\n",
    "    historical_val_costs[current_set_name]=val_costs\n",
    "            \n",
    "            \n",
    "train_writer.close()\n",
    "#valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_accs = []\n",
    "for i in range(0,num_classes/2):\n",
    "    set_acc =  historical_cross_test_acc['set4-set'+str(i)][-1]*100\n",
    "    print('Accuracy on set {}:{} after training set {}:{} is:{}'.format(i, sets[i],\\\n",
    "                                    4, sets[2], set_acc))\n",
    "    set_accs.append(set_acc)\n",
    "n_test_samples = np.array(n_test_samples)\n",
    "final_acc = (n_test_samples*set_accs).sum()/n_test_samples.sum()\n",
    "print('Final accuracy on all sets:{}'.format(final_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_w2.flatten(),bins=100,log=True)\n",
    "#plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_2.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_b2.flatten(),100,log=True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(w_3.eval().flatten(), 100)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_w3.flatten(),100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_3.eval().flatten(), 10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omega_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_big_omeg_b3.flatten(),10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "199.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
